{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pyijiezhang/cold-posteriors-and-pacbayes/blob/main/Yijie's_Variational_NNs_Keras_TFP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2EOnCShVSJ4"
      },
      "source": [
        "# Set-Up\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall tensorflow -y"
      ],
      "metadata": {
        "id": "4siUQ4-eU0qC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBfZZK217Xpp"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow==2.8.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip uninstall import tensorflow_probability -y"
      ],
      "metadata": {
        "id": "UYOX3i2JVG-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0o1OfUk9BQh"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow_probability==0.16.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiLjNLa2Fxln",
        "outputId": "91ae3f1a-7296-4bfe-dc3e-c40bef0c67aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kFbyHYemoTLm"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lXsO6OyvFiJz"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D, Lambda\n",
        "\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ezfO-JPQ9JNs"
      },
      "outputs": [],
      "source": [
        "assert(tf.test.gpu_device_name())\n",
        "tf.keras.backend.clear_session()\n",
        "tf.config.optimizer.set_jit(True) # Enable XLA."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pickle"
      ],
      "metadata": {
        "id": "rvBgUpdYhcTe"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhtuinGb7p7X",
        "outputId": "7ba5c0c0-dd83-42eb-dcb0-336e5f719cd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.11.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyjb_iYxcdCR"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SXIhRtCfxCFB"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "\n",
        "def load_data(data_set):\n",
        "  if data_set=='fashion_mnist':\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
        "  else:\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "  \n",
        "\n",
        "  tf.keras.utils.set_random_seed(123)\n",
        "  a = np.random.permutation(60000) #60000 original\n",
        "  x_train = x_train[a,...]\n",
        "  y_train = y_train[a]\n",
        "  # Add a new axis\n",
        "  x_train = x_train[..., np.newaxis]\n",
        "  x_test = x_test[..., np.newaxis]\n",
        "\n",
        "\n",
        "  # Convert class vectors to binary class matrices.\n",
        "  y_train = to_categorical(y_train, num_classes)\n",
        "  y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "  # Data normalization\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdc9t3j5os2T"
      },
      "source": [
        "# Prior Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi3i0Qya01CV"
      },
      "source": [
        "Define functions to initialize different types of prior distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1P33CQe3MYeo"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.python.distributions import normal as normal_lib\n",
        "from tensorflow_probability.python.distributions import independent as independent_lib\n",
        "from tensorflow_probability.python.distributions import kullback_leibler as kl_lib\n",
        "\n",
        "def make_normal_prior(scale):\n",
        "  \"\"\"\n",
        "  Defines a function that returns a Gaussian prior with the given standard\n",
        "  deviation.\n",
        "  \"\"\"\n",
        "  def _fn(dtype, shape, name, trainable,add_variable_fn):\n",
        "    del name, trainable, add_variable_fn   # unused\n",
        "    dist = normal_lib.Normal(loc=tf.zeros(shape, dtype), \n",
        "                             scale=dtype.as_numpy_dtype(scale))\n",
        "    batch_ndims = tf.size(dist.batch_shape_tensor())\n",
        "    return independent_lib.Independent(dist, \n",
        "                                       reinterpreted_batch_ndims=batch_ndims)\n",
        "\n",
        "  return _fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9E9K-6kuPq0"
      },
      "source": [
        "# Posteriors Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6FwtaZJ3EYu"
      },
      "source": [
        "Define posterior distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QoVqrmiDi5r1"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf1\n",
        "\n",
        "mean_field_init_untransformed_scale = -7.0\n",
        "\n",
        "# TODO(trandustin): Remove need for this boilerplate code.\n",
        "def mean_field_fn(empirical_bayes=False,\n",
        "                  initializer=tf1.initializers.he_normal()):\n",
        "  \"\"\"Constructors for Gaussian prior and posterior distributions.\n",
        "  Args:\n",
        "    empirical_bayes (bool): Whether to train the variance of the prior or not.\n",
        "    initializer (tf1.initializer): Initializer for the posterior means.\n",
        "  Returns:\n",
        "    prior, posterior (tfp.distribution): prior and posterior\n",
        "    to be fed into a Bayesian Layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def prior(dtype, shape, name, trainable, add_variable_fn):\n",
        "    \"\"\"Returns the prior distribution (tfp.distributions.Independent).\"\"\"\n",
        "    softplus_inverse_scale = np.log(np.exp(1.) - 1.)\n",
        "\n",
        "    istrainable = add_variable_fn(\n",
        "        name=name + '_istrainable',\n",
        "        shape=(),\n",
        "        initializer=tf1.constant_initializer(1.),\n",
        "        dtype=dtype,\n",
        "        trainable=False)\n",
        "\n",
        "    untransformed_scale = add_variable_fn(\n",
        "        name=name + '_untransformed_scale',\n",
        "        shape=(),\n",
        "        initializer=tf1.constant_initializer(softplus_inverse_scale),\n",
        "        dtype=dtype,\n",
        "        trainable=empirical_bayes and trainable)\n",
        "    scale = (\n",
        "        np.finfo(dtype.as_numpy_dtype).eps +\n",
        "        tf.nn.softplus(untransformed_scale * istrainable + (1. - istrainable) *\n",
        "                       tf1.stop_gradient(untransformed_scale)))\n",
        "    loc = add_variable_fn(\n",
        "        name=name + '_loc',\n",
        "        shape=shape,\n",
        "        initializer=tf1.constant_initializer(0.),\n",
        "        dtype=dtype,\n",
        "        trainable=False)\n",
        "    dist = tfp.distributions.Normal(loc=loc, scale=scale)\n",
        "    dist.istrainable = istrainable\n",
        "    dist.untransformed_scale = untransformed_scale\n",
        "    batch_ndims = tf1.size(input=dist.batch_shape_tensor())\n",
        "    return tfp.distributions.Independent(dist,\n",
        "                                         reinterpreted_batch_ndims=batch_ndims)\n",
        "\n",
        "  def posterior(dtype, shape, name, trainable, add_variable_fn):\n",
        "    \"\"\"Returns the posterior distribution (tfp.distributions.Independent).\"\"\"\n",
        "    untransformed_scale = add_variable_fn(\n",
        "        name=name + '_untransformed_scale',\n",
        "        shape=shape,\n",
        "        initializer=tf1.initializers.random_normal(\n",
        "            mean=mean_field_init_untransformed_scale, stddev=0.1),\n",
        "        dtype=dtype,\n",
        "        trainable=trainable)\n",
        "    scale = (\n",
        "        np.finfo(dtype.as_numpy_dtype).eps +\n",
        "        tf.nn.softplus(untransformed_scale))\n",
        "    loc = add_variable_fn(\n",
        "        name=name + '_loc',\n",
        "        shape=shape,\n",
        "        initializer=initializer,\n",
        "        dtype=dtype,\n",
        "        trainable=trainable)\n",
        "    dist = tfp.distributions.Normal(loc=loc, scale=scale)\n",
        "    dist.untransformed_scale = untransformed_scale\n",
        "    batch_ndims = tf1.size(input=dist.batch_shape_tensor())\n",
        "    return tfp.distributions.Independent(dist,\n",
        "                                         reinterpreted_batch_ndims=batch_ndims)\n",
        "\n",
        "  return prior, posterior\n",
        "\n",
        "prior_fn, posterior_fn = mean_field_fn(empirical_bayes=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "uKuT191d3Xzh"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.python.distributions import deterministic as deterministic_lib\n",
        "\n",
        "def make_normal_posterior(\n",
        "    is_singular=False,\n",
        "    loc_initializer=tf1.initializers.random_normal(stddev=0.1),\n",
        "    untransformed_scale_initializer=tf1.initializers.random_normal(\n",
        "        mean=-3., stddev=0.1),\n",
        "    loc_regularizer=None,\n",
        "    untransformed_scale_regularizer=None,\n",
        "    loc_constraint=None,\n",
        "    untransformed_scale_constraint=None):\n",
        "\n",
        "  loc_scale_fn = tfp.layers.util.default_loc_scale_fn(\n",
        "      is_singular=is_singular,\n",
        "      loc_initializer=loc_initializer,\n",
        "      untransformed_scale_initializer=untransformed_scale_initializer,\n",
        "      loc_regularizer=loc_regularizer,\n",
        "      untransformed_scale_regularizer=untransformed_scale_regularizer,\n",
        "      loc_constraint=loc_constraint,\n",
        "      untransformed_scale_constraint=untransformed_scale_constraint)\n",
        "  def _fn(dtype, shape, name, trainable, add_variable_fn):\n",
        "    loc, scale = loc_scale_fn(dtype, shape, name, trainable, add_variable_fn)\n",
        "    if scale is None:\n",
        "      dist = deterministic_lib.Deterministic(loc=loc)\n",
        "    else:\n",
        "      dist = normal_lib.Normal(loc=loc, scale=scale)\n",
        "    batch_ndims = tf.size(dist.batch_shape_tensor())\n",
        "    return independent_lib.Independent(\n",
        "        dist, reinterpreted_batch_ndims=batch_ndims)\n",
        "  return _fn\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "BW9rqXi-QlKc"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf1\n",
        "from tensorflow_probability.python.distributions import deterministic as deterministic_lib\n",
        "\n",
        "def default_mean_field_mixture_fn(\n",
        "    is_singular=False,\n",
        "    loc_initializer=tf1.initializers.random_normal(stddev=0.1),\n",
        "    untransformed_scale_initializer=tf1.initializers.random_normal(\n",
        "        mean=-3., stddev=0.1),\n",
        "    loc_regularizer=None,\n",
        "    untransformed_scale_regularizer=None,\n",
        "    loc_constraint=None,\n",
        "    untransformed_scale_constraint=None):\n",
        " \n",
        "  loc_scale_fn = tfp.layers.util.default_loc_scale_fn(\n",
        "      is_singular=is_singular,\n",
        "      loc_initializer=loc_initializer,\n",
        "      untransformed_scale_initializer=untransformed_scale_initializer,\n",
        "      loc_regularizer=loc_regularizer,\n",
        "      untransformed_scale_regularizer=untransformed_scale_regularizer,\n",
        "      loc_constraint=loc_constraint,\n",
        "      untransformed_scale_constraint=untransformed_scale_constraint)\n",
        "  def _fn(dtype, shape, name, trainable, add_variable_fn):\n",
        "    loc, scale = loc_scale_fn(dtype, shape, name, trainable, add_variable_fn)\n",
        "    #loc_2, scale_2 = loc_scale_fn(dtype, shape, name, trainable, add_variable_fn)\n",
        "    if scale is None:\n",
        "      dist = deterministic_lib.Deterministic(loc=loc)\n",
        "    else:\n",
        "      dist = tfp.distributions.Normal(loc=loc, scale=scale)\n",
        "      #dist2 = tfp.distributions.Normal(loc=tf.stop_gradient(loc_2), scale=scale_2)\n",
        "\n",
        "    batch_ndims = tf.size(dist.batch_shape_tensor())\n",
        "\n",
        "    return tfp.distributions.Mixture(\n",
        "        cat=tfp.distributions.Categorical(probs=[0.5, 0.5]),\n",
        "        components=[\n",
        "            tfp.distributions.Independent(dist,\n",
        "                            reinterpreted_batch_ndims=batch_ndims),\n",
        "            tfp.distributions.Independent(tfp.distributions.Normal(\n",
        "                loc=tf.zeros(shape, dtype=dtype), \n",
        "                scale=1.0*tf.ones(shape, dtype=dtype)),\n",
        "                            reinterpreted_batch_ndims=batch_ndims)],\n",
        "        name='spike_and_slab')\n",
        "\n",
        "  return _fn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR-MGa04cmVh"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJyqZswKTPil"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "VMA-Cx8KULte"
      },
      "outputs": [],
      "source": [
        "def conv2d(filters, kernel_size, padding='SAME', l2_val=2e-4):\n",
        "    return tf.keras.layers.Conv2D(\n",
        "          filters,\n",
        "          kernel_size,\n",
        "          padding=padding,\n",
        "          activation=tf.nn.relu,\n",
        "          bias_regularizer=tf.keras.regularizers.L2(l2_val),\n",
        "          kernel_regularizer=tf.keras.regularizers.L2(l2_val))\n",
        "\n",
        "def max_pool():\n",
        "    return tf.keras.layers.MaxPooling2D(\n",
        "        pool_size=[2, 2],\n",
        "        strides=[2, 2],\n",
        "        #padding='same',\n",
        "    )\n",
        "def avg_pool():\n",
        "    return tf.keras.layers.AveragePooling2D(\n",
        "        pool_size=[2, 2],\n",
        "        strides=[2, 2],\n",
        "        #padding='same',\n",
        "    )\n",
        "\n",
        "def dense(units, activation, l2_val=2e-4):\n",
        "    return tf.keras.layers.Dense(\n",
        "        units,\n",
        "        activation,\n",
        "        bias_regularizer=tf.keras.regularizers.L2(l2_val),\n",
        "        kernel_regularizer=tf.keras.regularizers.L2(l2_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "oCbBDK6PTRNf"
      },
      "outputs": [],
      "source": [
        "def conv2d_variational(filters, kernel_size, strides=(1, 1), padding='SAME', kernel_posterior_fn=None, kernel_prior_fn=None, bias_prior_fn=None, kernel_divergence_fn=None):\n",
        "    \"\"\"Convenience wrapper for conv layers.\"\"\"\n",
        "    if bias_prior_fn==None:\n",
        "      bias_prior_fn = kernel_prior_fn\n",
        "    \n",
        "    return tfp.layers.Convolution2DReparameterization(\n",
        "          filters,\n",
        "          kernel_size,\n",
        "          padding=padding,\n",
        "          activation=tf.nn.relu,\n",
        "          kernel_posterior_fn=kernel_posterior_fn,\n",
        "          kernel_prior_fn=kernel_prior_fn,\n",
        "          kernel_divergence_fn=kernel_divergence_fn,\n",
        "          bias_posterior_fn = kernel_posterior_fn,\n",
        "          bias_prior_fn = bias_prior_fn,\n",
        "          bias_divergence_fn=kernel_divergence_fn)\n",
        "\n",
        "\n",
        "def dense_variational(units, activation, kernel_posterior_fn=None, kernel_prior_fn=None, bias_prior_fn=None, kernel_divergence_fn=None):\n",
        "    if bias_prior_fn==None:\n",
        "      bias_prior_fn = kernel_prior_fn\n",
        "\n",
        "    return tfp.layers.DenseReparameterization(\n",
        "        units,\n",
        "        activation,\n",
        "        kernel_posterior_fn=kernel_posterior_fn,\n",
        "        kernel_prior_fn=kernel_prior_fn,\n",
        "        kernel_divergence_fn=kernel_divergence_fn,\n",
        "        bias_posterior_fn = kernel_posterior_fn,\n",
        "        bias_prior_fn = bias_prior_fn,\n",
        "        bias_divergence_fn=kernel_divergence_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHhNiKBIwziq"
      },
      "source": [
        "## LeNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMo13XRwNtzv"
      },
      "source": [
        "### Variational"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "RE-IGUORIEDn"
      },
      "outputs": [],
      "source": [
        "class LeNet_Variational(Sequential):\n",
        "  def __init__(self, input_shape, nb_classes,\n",
        "               kernel_posterior_fn=make_normal_posterior(),\n",
        "               kernel_prior_fn=make_normal_prior(1.0),\n",
        "               kernel_divergence_fn=lambda q, p, ignore: kl_lib.kl_divergence(q, p)/(x_train.shape[0]*lamb),\n",
        "               gamma=1.0):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.weight_layers = [0, 2, 4, 6, 7]\n",
        "\n",
        "    self.add(tf.keras.layers.Input(shape=input_shape))\n",
        "\n",
        "    self.add(conv2d_variational(6, 5, padding = \"same\", kernel_posterior_fn=kernel_posterior_fn, \n",
        "                                kernel_prior_fn=kernel_prior_fn, \n",
        "                                kernel_divergence_fn=kernel_divergence_fn))\n",
        "    self.add(avg_pool())\n",
        "\n",
        "    self.add(conv2d_variational(16, 5, padding=\"same\", kernel_posterior_fn=kernel_posterior_fn, \n",
        "                                kernel_prior_fn=kernel_prior_fn, \n",
        "                                kernel_divergence_fn=kernel_divergence_fn))\n",
        "    self.add(avg_pool())\n",
        "\n",
        "    self.add(Flatten())\n",
        "\n",
        "\n",
        "    self.add(dense_variational(120, tf.nn.relu, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "\n",
        "\n",
        "    self.add(dense_variational(84, tf.nn.relu, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "\n",
        "    self.add(dense_variational(nb_classes, None, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "    \n",
        "    self.add(Lambda(lambda x: x*gamma))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_variational = LeNet_Variational(x_train[0].shape, num_classes)\n",
        "model_variational.summary()"
      ],
      "metadata": {
        "id": "9r6WjDAwXU6h",
        "outputId": "a6e22410-137e-4a53-d8b8-fd925ff4c209",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"le_net__variational_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_reparameterization_3  (None, 28, 28, 6)        312       \n",
            "  (Conv2DReparameterization)                                     \n",
            "                                                                 \n",
            " average_pooling2d (AverageP  (None, 14, 14, 6)        0         \n",
            " ooling2D)                                                       \n",
            "                                                                 \n",
            " conv2d_reparameterization_4  (None, 14, 14, 16)       4832      \n",
            "  (Conv2DReparameterization)                                     \n",
            "                                                                 \n",
            " average_pooling2d_1 (Averag  (None, 7, 7, 16)         0         \n",
            " ePooling2D)                                                     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense_reparameterization (D  (None, 120)              188400    \n",
            " enseReparameterization)                                         \n",
            "                                                                 \n",
            " dense_reparameterization_1   (None, 84)               20328     \n",
            " (DenseReparameterization)                                       \n",
            "                                                                 \n",
            " dense_reparameterization_2   (None, 10)               1700      \n",
            " (DenseReparameterization)                                       \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 215,572\n",
            "Trainable params: 215,572\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwGNoPYQQdNE"
      },
      "source": [
        "# Training Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qXUVl6tWoO00"
      },
      "outputs": [],
      "source": [
        "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.0)\n",
        "\n",
        "@tf.function\n",
        "def ece(y_true,y_pred):\n",
        "  return tfp.stats.expected_calibration_error(10,logits=y_pred, labels_true=tf.argmax(y_true,axis=1))\n",
        "\n",
        "# Place the logs in a timestamped subdirectory\n",
        "# This allows to easy select different training runs\n",
        "# In order not to overwrite some data, it is useful to have a name with a timestamp\n",
        "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Specify the callback object\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# tf.keras.callback.TensorBoard ensures that logs are created and stored\n",
        "# We need to pass callback object to the fit method\n",
        "# The way to do this is by passing the list of callback objects, which is in our case just one\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_categorical_crossentropy\", patience=10, restore_best_weights=True)\n",
        "\n",
        "adam_opt = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "def schedule(epoch, lr):\n",
        "  if (epoch<25):\n",
        "    return 0.001\n",
        "  elif epoch<50:\n",
        "    return 0.0001\n",
        "  else:\n",
        "    return 0.00001\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iCCKNSDXfTis"
      },
      "outputs": [],
      "source": [
        "class BMA_Model(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, model, mc_samples):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.mc_samples = mc_samples\n",
        "  \n",
        "  def call(self, inputs, training=False):\n",
        "    list_models =[self.model(inputs) for i in range(self.mc_samples)]\n",
        "    stack_models = tf.stack(list_models,axis=1)\n",
        "    return tfp.math.reduce_logmeanexp(stack_models,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPt69y9ZcusY"
      },
      "source": [
        "# Variational Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7OWzof1fmtBb"
      },
      "outputs": [],
      "source": [
        "# experiment settings\n",
        "data_use=\"mnist\"\n",
        "model_used=\"lenet\"\n",
        "num_posterior_samples=10\n",
        "n_epochs=300\n",
        "batch_size=1000\n",
        "lr=0.001\n",
        "\n",
        "seeds=[15,24]\n",
        "\n",
        "# lambs for likelihood tempering\n",
        "lambs=np.linspace(0.5, 3.5, 7)\n",
        "\n",
        "# label_smoothing\n",
        "# label_smoothing=0.0\n",
        "# cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=label_smoothing)\n",
        "\n",
        "# label noise\n",
        "label_noises=[0.0] #0.1,0.2,0.3]\n",
        "\n",
        "# smooth softmax\n",
        "gammas=[1.0] #,3.0,5.0,10.0]\n",
        "\n",
        "# data augmentation\n",
        "if_das=[True]\n",
        "\n",
        "# prior\n",
        "prior_scales=[0.01]#,0.1,1.0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RbTGSR9jQje6"
      },
      "outputs": [],
      "source": [
        "# x_train_fashion, y_train_fashion, x_test_fashion, y_test_fashion = load_data('fashion_mnist')\n",
        "x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist = load_data('mnist')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "MjOWNM9Cla0u"
      },
      "outputs": [],
      "source": [
        "# if if_da:\n",
        "#   from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#   datagen = ImageDataGenerator(\n",
        "#           rotation_range=10,  # Rotate the image randomly by up to 10 degrees\n",
        "#           zoom_range=0.1,     # Zoom in or out of the image randomly by up to 10%\n",
        "#           width_shift_range=0.1,  # Shift the image horizontally by up to 10%\n",
        "#           height_shift_range=0.1, # Shift the image vertically by up to 10%\n",
        "#           )\n",
        "#   datagen.fit(x_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TB60tuuQzZ3q"
      },
      "outputs": [],
      "source": [
        "# if if_da:\n",
        "#   from tensorflow import keras\n",
        "#   from tensorflow.keras import layers\n",
        "\n",
        "#   # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
        "#   data_augmentation = keras.Sequential(\n",
        "#       [\n",
        "#           layers.RandomFlip(\"horizontal\"),\n",
        "#           layers.RandomRotation(0.1),\n",
        "#           layers.RandomZoom(0.1),\n",
        "#       ]\n",
        "#   )\n",
        "\n",
        "#   input_shape = x_train.shape[1:]\n",
        "#   classes = 10\n",
        "\n",
        "#   # Create a tf.data pipeline of augmented images (and their labels)\n",
        "#   train_augmented = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "#   train_augmented = train_augmented.batch(batch_size).map(lambda x, y: (data_augmentation(x), y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "n4i8AnQ6Qt-3",
        "outputId": "3830ee55-3eb2-4922-dee1-08bb40a440d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
            "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "0.5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4585300b691b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             model_variational = LeNet_Variational(input_shape, num_classes,\n\u001b[0m\u001b[1;32m     72\u001b[0m                                                       \u001b[0mkernel_prior_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_normal_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior_scale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                                                       \u001b[0mkernel_divergence_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkl_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-19-d7ecdb7609b0>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_shape, nb_classes, kernel_posterior_fn, kernel_prior_fn, kernel_divergence_fn, gamma)\u001b[0m\n\u001b[1;32m     15\u001b[0m                                 \u001b[0mkernel_prior_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_prior_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                 kernel_divergence_fn=kernel_divergence_fn))\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     self.add(conv2d_variational(16, 5, padding=\"same\", kernel_posterior_fn=kernel_posterior_fn, \n",
            "\u001b[0;32m<ipython-input-12-1a34c86a4ebc>\u001b[0m in \u001b[0;36mavg_pool\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     )\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     return tf.keras.layers.AvgPooling2D(\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.layers' has no attribute 'AvgPooling2D'"
          ]
        }
      ],
      "source": [
        "model_variational=[]\n",
        "for prior_scale in prior_scales: \n",
        "  for gamma in gammas:\n",
        "    for label_noise in label_noises:\n",
        "      for if_da in if_das:\n",
        "\n",
        "        setting_name=f\"prior_scale_{prior_scale}_label_noise_{label_noise}_smooth_softmax_{gamma}_data_augmentation_{if_da}\" \n",
        "\n",
        "        x_train = x_train_mnist\n",
        "        y_train = y_train_mnist\n",
        "        x_test = x_test_mnist\n",
        "        y_test = y_test_mnist\n",
        "\n",
        "        input_shape=x_train[0].shape\n",
        "\n",
        "        if label_noise > 0.0:\n",
        "          import random\n",
        "          # train\n",
        "          ind_noisy=random.sample(range(1, y_train.shape[0]), int(y_train.shape[0]*label_noise))\n",
        "          for ind in ind_noisy:\n",
        "            false_label=random.sample(range(1,y_train.shape[1]),1)[0]\n",
        "            while y_train[ind, false_label] != 1.0:\n",
        "              temp=np.zeros((y_train.shape[1]))\n",
        "              temp[false_label]=1.0\n",
        "              y_train[ind, :]=temp\n",
        "              break\n",
        "            else:\n",
        "              false_label=random.sample(range(1,y_train.shape[1]),1)[0]\n",
        "          # test\n",
        "          ind_noisy=random.sample(range(1, y_test.shape[0]), int(y_test.shape[0]*label_noise))\n",
        "          for ind in ind_noisy:\n",
        "            false_label=random.sample(range(1,y_test.shape[1]),1)[0]\n",
        "            while y_test[ind, false_label] != 1.0:\n",
        "              temp=np.zeros((y_test.shape[1]))\n",
        "              temp[false_label]=1.0\n",
        "              y_test[ind, :]=temp\n",
        "              break\n",
        "            else:\n",
        "              false_label=random.sample(range(1,y_test.shape[1]),1)[0]\n",
        "\n",
        "        if if_da:\n",
        "          from tensorflow import keras\n",
        "          from tensorflow.keras import layers\n",
        "\n",
        "          # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
        "          data_augmentation = keras.Sequential(\n",
        "              [\n",
        "                  layers.RandomFlip(\"horizontal\"),\n",
        "                  layers.RandomRotation(0.1),\n",
        "                  layers.RandomZoom(0.1),\n",
        "              ]\n",
        "          )\n",
        "\n",
        "          input_shape = x_train.shape[1:]\n",
        "\n",
        "          # Create a tf.data pipeline of augmented images (and their labels)\n",
        "          train_augmented = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "          train_augmented = train_augmented.batch(batch_size).map(lambda x, y: (data_augmentation(x), y))\n",
        "        \n",
        "        results={}\n",
        "        for seed in seeds:\n",
        "          print(seed)\n",
        "          results[seed]={} # a dict, with the structure, dict_results[lamb]=[log_ps_train, log_ps_test, metrics_bma]\n",
        "          for lamb in lambs:\n",
        "            \n",
        "            print(lamb)\n",
        "            results[seed][lamb]=[]\n",
        "\n",
        "            tf.keras.utils.set_random_seed(seed)\n",
        "\n",
        "            model_variational = LeNet_Variational(input_shape, num_classes,\n",
        "                                                      kernel_prior_fn=make_normal_prior(prior_scale),\n",
        "                                                      kernel_divergence_fn=lambda q, p, ignore: kl_lib.kl_divergence(q, p)/(x_train.shape[0]*lamb),\n",
        "                                                      gamma=gamma)\n",
        "\n",
        "            model_variational.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
        "                            loss = cce,\n",
        "                            metrics=['accuracy', cce]\n",
        "                            )\n",
        "\n",
        "            if if_da:\n",
        "              # model_variational.fit(datagen.flow(x_train, y_train, batch_size=batch_size), \n",
        "              #           epochs=n_epochs, \n",
        "              #           verbose = 1)\n",
        "              model_variational.fit(train_augmented, \n",
        "                        epochs=n_epochs, \n",
        "                        # workers=8,\n",
        "                        verbose = 1)\n",
        "            else:\n",
        "              model_variational.fit(x_train, y=y_train, \n",
        "                        epochs=n_epochs, \n",
        "                        batch_size = batch_size,\n",
        "                        verbose = 1)\n",
        "            \n",
        "            log_ps_train=[]\n",
        "            log_ps_test=[]\n",
        "            metrics_gibbs_train=[]\n",
        "            metrics_gibbs_test=[]\n",
        "            metrics_bayes_train=[]\n",
        "            metrics_bayes_test=[]\n",
        "\n",
        "            # loop through posterior samples\n",
        "            for i in range(num_posterior_samples):\n",
        "              \n",
        "              # compute the categorical dist. and log_p for train data\n",
        "              p_train=tfp.distributions.Categorical(logits=model_variational(x_train))\n",
        "              log_p_train=p_train.log_prob(tf.argmax(y_train, axis=1))\n",
        "              log_ps_train.append(log_p_train)\n",
        "              \n",
        "              # compute the categorical dist. and log_p for test data\n",
        "              p_test=tfp.distributions.Categorical(logits=model_variational(x_test)) \n",
        "              log_p_test=p_test.log_prob(tf.argmax(y_test, axis=1))\n",
        "              log_ps_test.append(log_p_test)\n",
        "\n",
        "              # compute gibbs-based metrics\n",
        "              metrics_gibbs_train.append(model_variational.evaluate(x_train, y_train, batch_size=batch_size))\n",
        "              metrics_gibbs_test.append(model_variational.evaluate(x_test, y_test, batch_size=batch_size))\n",
        "\n",
        "            # compute bayes-based metric\n",
        "            bma_model=BMA_Model(model_variational, num_posterior_samples)\n",
        "            bma_model.compile(metrics=['accuracy', cce])\n",
        "            metrics_bayes_train.append(bma_model.evaluate(x_train, y_train, batch_size=batch_size))\n",
        "            metrics_bayes_test.append(bma_model.evaluate(x_test, y_test, batch_size=batch_size))\n",
        "\n",
        "            !nvidia-smi\n",
        "            \n",
        "            # save log_p for train data\n",
        "            results[seed][lamb].append(tf.stack(log_ps_train,axis=1).numpy())\n",
        "            # save log_p for test data \n",
        "            results[seed][lamb].append(tf.stack(log_ps_test,axis=1).numpy())\n",
        "            # save gibbs-based metric\n",
        "            results[seed][lamb].append(metrics_gibbs_train)\n",
        "            results[seed][lamb].append(metrics_gibbs_test)\n",
        "            # save bayes-based metric\n",
        "            results[seed][lamb].append(metrics_bayes_train)\n",
        "            results[seed][lamb].append(metrics_bayes_test)\n",
        "\n",
        "        with open(f'{setting_name}.pickle', 'wb') as handle:\n",
        "          pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_variational.summary()"
      ],
      "metadata": {
        "id": "hgXENp57XfDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "7bAlp9m5Zg9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJEY_ldvaQ83"
      },
      "outputs": [],
      "source": [
        "# files.download('prior_scale_1.0_label_noise_0.0_smooth_softmax_3.0_data_augmentation_False.pickle') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "itHnSRJmgOXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqmjiCxDXxUO"
      },
      "outputs": [],
      "source": [
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}