{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pyijiezhang/cold-posteriors-and-pacbayes/blob/main/Yijie's_Variational_NNs_Keras_TFP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2EOnCShVSJ4"
      },
      "source": [
        "# Set-Up\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow -y"
      ],
      "metadata": {
        "id": "4siUQ4-eU0qC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efaacb6a-3826-4e6a-efde-36e4126e8046"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xBfZZK217Xpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4f5832-e254-4767-d836-538ece787303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8.3\n",
            "  Using cached tensorflow-2.8.3-cp38-cp38-manylinux2010_x86_64.whl (498.4 MB)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (1.1.2)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (2.8.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (2.8.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (4.5.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (3.19.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (1.51.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (2.2.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (0.31.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (1.4.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (1.6.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (57.4.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (23.1.21)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (1.22.4)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.8.3) (15.0.6.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.3) (0.38.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.2.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.16.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.25.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (6.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (1.26.14)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (3.2.2)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.8.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.8.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall import tensorflow_probability -y"
      ],
      "metadata": {
        "id": "UYOX3i2JVG-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9a1756a-225a-475b-e99b-d5321e2dc9e5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping import as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: tensorflow-probability 0.16.0\n",
            "Uninstalling tensorflow-probability-0.16.0:\n",
            "  Successfully uninstalled tensorflow-probability-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u0o1OfUk9BQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9ae294-fc8e-4a57-89f3-e7bcebac806d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_probability==0.16.0\n",
            "  Using cached tensorflow_probability-0.16.0-py2.py3-none-any.whl (6.3 MB)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (0.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (1.15.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (1.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (4.4.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (1.22.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (0.1.8)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow_probability==0.16.0) (2.2.1)\n",
            "Installing collected packages: tensorflow_probability\n",
            "Successfully installed tensorflow_probability-0.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_probability==0.16.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiLjNLa2Fxln",
        "outputId": "994f685d-e108-451b-9c8e-fdfdcc106d85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kFbyHYemoTLm"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "lXsO6OyvFiJz"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D, Lambda\n",
        "\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ezfO-JPQ9JNs"
      },
      "outputs": [],
      "source": [
        "assert(tf.test.gpu_device_name())\n",
        "tf.keras.backend.clear_session()\n",
        "tf.config.optimizer.set_jit(True) # Enable XLA."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pickle"
      ],
      "metadata": {
        "id": "rvBgUpdYhcTe"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhtuinGb7p7X",
        "outputId": "4bd51615-d408-479c-d114-2c4058bf628e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.3\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyjb_iYxcdCR"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SXIhRtCfxCFB"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "\n",
        "def load_data(data_set):\n",
        "  if data_set=='fashion_mnist':\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
        "  else:\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "  \n",
        "\n",
        "  tf.keras.utils.set_random_seed(123)\n",
        "  a = np.random.permutation(60000) #60000 original\n",
        "  x_train = x_train[a,...]\n",
        "  y_train = y_train[a]\n",
        "  # Add a new axis\n",
        "  x_train = x_train[..., np.newaxis]\n",
        "  x_test = x_test[..., np.newaxis]\n",
        "\n",
        "\n",
        "  # Convert class vectors to binary class matrices.\n",
        "  y_train = to_categorical(y_train, num_classes)\n",
        "  y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "  # Data normalization\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdc9t3j5os2T"
      },
      "source": [
        "# Prior Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi3i0Qya01CV"
      },
      "source": [
        "Define functions to initialize different types of prior distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1P33CQe3MYeo"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.python.distributions import normal as normal_lib\n",
        "from tensorflow_probability.python.distributions import independent as independent_lib\n",
        "from tensorflow_probability.python.distributions import kullback_leibler as kl_lib\n",
        "\n",
        "def make_normal_prior(scale):\n",
        "  \"\"\"\n",
        "  Defines a function that returns a Gaussian prior with the given standard\n",
        "  deviation.\n",
        "  \"\"\"\n",
        "  def _fn(dtype, shape, name, trainable,add_variable_fn):\n",
        "    del name, trainable, add_variable_fn   # unused\n",
        "    dist = normal_lib.Normal(loc=tf.zeros(shape, dtype), \n",
        "                             scale=dtype.as_numpy_dtype(scale))\n",
        "    batch_ndims = tf.size(dist.batch_shape_tensor())\n",
        "    return independent_lib.Independent(dist, \n",
        "                                       reinterpreted_batch_ndims=batch_ndims)\n",
        "\n",
        "  return _fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9E9K-6kuPq0"
      },
      "source": [
        "# Posteriors Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6FwtaZJ3EYu"
      },
      "source": [
        "Define posterior distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QoVqrmiDi5r1"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf1\n",
        "\n",
        "mean_field_init_untransformed_scale = -7.0\n",
        "\n",
        "# TODO(trandustin): Remove need for this boilerplate code.\n",
        "def mean_field_fn(empirical_bayes=False,\n",
        "                  initializer=tf1.initializers.he_normal()):\n",
        "  \"\"\"Constructors for Gaussian prior and posterior distributions.\n",
        "  Args:\n",
        "    empirical_bayes (bool): Whether to train the variance of the prior or not.\n",
        "    initializer (tf1.initializer): Initializer for the posterior means.\n",
        "  Returns:\n",
        "    prior, posterior (tfp.distribution): prior and posterior\n",
        "    to be fed into a Bayesian Layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def prior(dtype, shape, name, trainable, add_variable_fn):\n",
        "    \"\"\"Returns the prior distribution (tfp.distributions.Independent).\"\"\"\n",
        "    softplus_inverse_scale = np.log(np.exp(1.) - 1.)\n",
        "\n",
        "    istrainable = add_variable_fn(\n",
        "        name=name + '_istrainable',\n",
        "        shape=(),\n",
        "        initializer=tf1.constant_initializer(1.),\n",
        "        dtype=dtype,\n",
        "        trainable=False)\n",
        "\n",
        "    untransformed_scale = add_variable_fn(\n",
        "        name=name + '_untransformed_scale',\n",
        "        shape=(),\n",
        "        initializer=tf1.constant_initializer(softplus_inverse_scale),\n",
        "        dtype=dtype,\n",
        "        trainable=empirical_bayes and trainable)\n",
        "    scale = (\n",
        "        np.finfo(dtype.as_numpy_dtype).eps +\n",
        "        tf.nn.softplus(untransformed_scale * istrainable + (1. - istrainable) *\n",
        "                       tf1.stop_gradient(untransformed_scale)))\n",
        "    loc = add_variable_fn(\n",
        "        name=name + '_loc',\n",
        "        shape=shape,\n",
        "        initializer=tf1.constant_initializer(0.),\n",
        "        dtype=dtype,\n",
        "        trainable=False)\n",
        "    dist = tfp.distributions.Normal(loc=loc, scale=scale)\n",
        "    dist.istrainable = istrainable\n",
        "    dist.untransformed_scale = untransformed_scale\n",
        "    batch_ndims = tf1.size(input=dist.batch_shape_tensor())\n",
        "    return tfp.distributions.Independent(dist,\n",
        "                                         reinterpreted_batch_ndims=batch_ndims)\n",
        "\n",
        "  def posterior(dtype, shape, name, trainable, add_variable_fn):\n",
        "    \"\"\"Returns the posterior distribution (tfp.distributions.Independent).\"\"\"\n",
        "    untransformed_scale = add_variable_fn(\n",
        "        name=name + '_untransformed_scale',\n",
        "        shape=shape,\n",
        "        initializer=tf1.initializers.random_normal(\n",
        "            mean=mean_field_init_untransformed_scale, stddev=0.1),\n",
        "        dtype=dtype,\n",
        "        trainable=trainable)\n",
        "    scale = (\n",
        "        np.finfo(dtype.as_numpy_dtype).eps +\n",
        "        tf.nn.softplus(untransformed_scale))\n",
        "    loc = add_variable_fn(\n",
        "        name=name + '_loc',\n",
        "        shape=shape,\n",
        "        initializer=initializer,\n",
        "        dtype=dtype,\n",
        "        trainable=trainable)\n",
        "    dist = tfp.distributions.Normal(loc=loc, scale=scale)\n",
        "    dist.untransformed_scale = untransformed_scale\n",
        "    batch_ndims = tf1.size(input=dist.batch_shape_tensor())\n",
        "    return tfp.distributions.Independent(dist,\n",
        "                                         reinterpreted_batch_ndims=batch_ndims)\n",
        "\n",
        "  return prior, posterior\n",
        "\n",
        "prior_fn, posterior_fn = mean_field_fn(empirical_bayes=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uKuT191d3Xzh"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.python.distributions import deterministic as deterministic_lib\n",
        "\n",
        "def make_normal_posterior(\n",
        "    is_singular=False,\n",
        "    loc_initializer=tf1.initializers.random_normal(stddev=0.1),\n",
        "    untransformed_scale_initializer=tf1.initializers.random_normal(\n",
        "        mean=-3., stddev=0.1),\n",
        "    loc_regularizer=None,\n",
        "    untransformed_scale_regularizer=None,\n",
        "    loc_constraint=None,\n",
        "    untransformed_scale_constraint=None):\n",
        "\n",
        "  loc_scale_fn = tfp.layers.util.default_loc_scale_fn(\n",
        "      is_singular=is_singular,\n",
        "      loc_initializer=loc_initializer,\n",
        "      untransformed_scale_initializer=untransformed_scale_initializer,\n",
        "      loc_regularizer=loc_regularizer,\n",
        "      untransformed_scale_regularizer=untransformed_scale_regularizer,\n",
        "      loc_constraint=loc_constraint,\n",
        "      untransformed_scale_constraint=untransformed_scale_constraint)\n",
        "  def _fn(dtype, shape, name, trainable, add_variable_fn):\n",
        "    loc, scale = loc_scale_fn(dtype, shape, name, trainable, add_variable_fn)\n",
        "    if scale is None:\n",
        "      dist = deterministic_lib.Deterministic(loc=loc)\n",
        "    else:\n",
        "      dist = normal_lib.Normal(loc=loc, scale=scale)\n",
        "    batch_ndims = tf.size(dist.batch_shape_tensor())\n",
        "    return independent_lib.Independent(\n",
        "        dist, reinterpreted_batch_ndims=batch_ndims)\n",
        "  return _fn\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BW9rqXi-QlKc"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf1\n",
        "from tensorflow_probability.python.distributions import deterministic as deterministic_lib\n",
        "\n",
        "def default_mean_field_mixture_fn(\n",
        "    is_singular=False,\n",
        "    loc_initializer=tf1.initializers.random_normal(stddev=0.1),\n",
        "    untransformed_scale_initializer=tf1.initializers.random_normal(\n",
        "        mean=-3., stddev=0.1),\n",
        "    loc_regularizer=None,\n",
        "    untransformed_scale_regularizer=None,\n",
        "    loc_constraint=None,\n",
        "    untransformed_scale_constraint=None):\n",
        " \n",
        "  loc_scale_fn = tfp.layers.util.default_loc_scale_fn(\n",
        "      is_singular=is_singular,\n",
        "      loc_initializer=loc_initializer,\n",
        "      untransformed_scale_initializer=untransformed_scale_initializer,\n",
        "      loc_regularizer=loc_regularizer,\n",
        "      untransformed_scale_regularizer=untransformed_scale_regularizer,\n",
        "      loc_constraint=loc_constraint,\n",
        "      untransformed_scale_constraint=untransformed_scale_constraint)\n",
        "  def _fn(dtype, shape, name, trainable, add_variable_fn):\n",
        "    loc, scale = loc_scale_fn(dtype, shape, name, trainable, add_variable_fn)\n",
        "    #loc_2, scale_2 = loc_scale_fn(dtype, shape, name, trainable, add_variable_fn)\n",
        "    if scale is None:\n",
        "      dist = deterministic_lib.Deterministic(loc=loc)\n",
        "    else:\n",
        "      dist = tfp.distributions.Normal(loc=loc, scale=scale)\n",
        "      #dist2 = tfp.distributions.Normal(loc=tf.stop_gradient(loc_2), scale=scale_2)\n",
        "\n",
        "    batch_ndims = tf.size(dist.batch_shape_tensor())\n",
        "\n",
        "    return tfp.distributions.Mixture(\n",
        "        cat=tfp.distributions.Categorical(probs=[0.5, 0.5]),\n",
        "        components=[\n",
        "            tfp.distributions.Independent(dist,\n",
        "                            reinterpreted_batch_ndims=batch_ndims),\n",
        "            tfp.distributions.Independent(tfp.distributions.Normal(\n",
        "                loc=tf.zeros(shape, dtype=dtype), \n",
        "                scale=1.0*tf.ones(shape, dtype=dtype)),\n",
        "                            reinterpreted_batch_ndims=batch_ndims)],\n",
        "        name='spike_and_slab')\n",
        "\n",
        "  return _fn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR-MGa04cmVh"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJyqZswKTPil"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "VMA-Cx8KULte"
      },
      "outputs": [],
      "source": [
        "def conv2d(filters, kernel_size, padding='SAME', l2_val=2e-4):\n",
        "    return tf.keras.layers.Conv2D(\n",
        "          filters,\n",
        "          kernel_size,\n",
        "          padding=padding,\n",
        "          activation=tf.nn.relu,\n",
        "          bias_regularizer=tf.keras.regularizers.L2(l2_val),\n",
        "          kernel_regularizer=tf.keras.regularizers.L2(l2_val))\n",
        "\n",
        "def max_pool():\n",
        "    return tf.keras.layers.MaxPooling2D(\n",
        "        pool_size=[2, 2],\n",
        "        strides=[2, 2],\n",
        "        #padding='same',\n",
        "    )\n",
        "def avg_pool():\n",
        "    return tf.keras.layers.AveragePooling2D(\n",
        "        pool_size=[2, 2],\n",
        "        strides=[2, 2],\n",
        "        #padding='same',\n",
        "    )\n",
        "\n",
        "def dense(units, activation, l2_val=2e-4):\n",
        "    return tf.keras.layers.Dense(\n",
        "        units,\n",
        "        activation,\n",
        "        bias_regularizer=tf.keras.regularizers.L2(l2_val),\n",
        "        kernel_regularizer=tf.keras.regularizers.L2(l2_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "oCbBDK6PTRNf"
      },
      "outputs": [],
      "source": [
        "def conv2d_variational(filters, kernel_size, strides=(1, 1), padding='SAME', kernel_posterior_fn=None, kernel_prior_fn=None, bias_prior_fn=None, kernel_divergence_fn=None):\n",
        "    \"\"\"Convenience wrapper for conv layers.\"\"\"\n",
        "    if bias_prior_fn==None:\n",
        "      bias_prior_fn = kernel_prior_fn\n",
        "    \n",
        "    return tfp.layers.Convolution2DReparameterization(\n",
        "          filters,\n",
        "          kernel_size,\n",
        "          padding=padding,\n",
        "          activation=tf.nn.relu,\n",
        "          kernel_posterior_fn=kernel_posterior_fn,\n",
        "          kernel_prior_fn=kernel_prior_fn,\n",
        "          kernel_divergence_fn=kernel_divergence_fn,\n",
        "          bias_posterior_fn = kernel_posterior_fn,\n",
        "          bias_prior_fn = bias_prior_fn,\n",
        "          bias_divergence_fn=kernel_divergence_fn)\n",
        "\n",
        "\n",
        "def dense_variational(units, activation, kernel_posterior_fn=None, kernel_prior_fn=None, bias_prior_fn=None, kernel_divergence_fn=None):\n",
        "    if bias_prior_fn==None:\n",
        "      bias_prior_fn = kernel_prior_fn\n",
        "\n",
        "    return tfp.layers.DenseReparameterization(\n",
        "        units,\n",
        "        activation,\n",
        "        kernel_posterior_fn=kernel_posterior_fn,\n",
        "        kernel_prior_fn=kernel_prior_fn,\n",
        "        kernel_divergence_fn=kernel_divergence_fn,\n",
        "        bias_posterior_fn = kernel_posterior_fn,\n",
        "        bias_prior_fn = bias_prior_fn,\n",
        "        bias_divergence_fn=kernel_divergence_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHhNiKBIwziq"
      },
      "source": [
        "## LeNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMo13XRwNtzv"
      },
      "source": [
        "### Variational"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "RE-IGUORIEDn"
      },
      "outputs": [],
      "source": [
        "class LeNet_Variational(Sequential):\n",
        "  def __init__(self, input_shape, nb_classes,\n",
        "               kernel_posterior_fn=make_normal_posterior(),\n",
        "               kernel_prior_fn=make_normal_prior(1.0),\n",
        "               kernel_divergence_fn=lambda q, p, ignore: kl_lib.kl_divergence(q, p)/(x_train.shape[0]*lamb),\n",
        "               gamma=1.0):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.weight_layers = [0, 2, 4, 6, 7]\n",
        "\n",
        "    self.add(tf.keras.layers.Input(shape=input_shape))\n",
        "\n",
        "    self.add(conv2d_variational(6, 5, padding = \"same\", kernel_posterior_fn=kernel_posterior_fn, \n",
        "                                kernel_prior_fn=kernel_prior_fn, \n",
        "                                kernel_divergence_fn=kernel_divergence_fn))\n",
        "    self.add(avg_pool())\n",
        "\n",
        "    self.add(conv2d_variational(16, 5, padding=\"same\", kernel_posterior_fn=kernel_posterior_fn, \n",
        "                                kernel_prior_fn=kernel_prior_fn, \n",
        "                                kernel_divergence_fn=kernel_divergence_fn))\n",
        "    self.add(avg_pool())\n",
        "\n",
        "    self.add(Flatten())\n",
        "\n",
        "\n",
        "    self.add(dense_variational(120, tf.nn.relu, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "\n",
        "\n",
        "    self.add(dense_variational(84, tf.nn.relu, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "\n",
        "    self.add(dense_variational(nb_classes, None, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "    \n",
        "    self.add(Lambda(lambda x: x*gamma))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet_Variational_Large(Sequential):\n",
        "  def __init__(self, input_shape, nb_classes,\n",
        "               kernel_posterior_fn=make_normal_posterior(),\n",
        "               kernel_prior_fn=make_normal_prior(1.0),\n",
        "               kernel_divergence_fn=lambda q, p, ignore: kl_lib.kl_divergence(q, p)/(x_train.shape[0]*lamb),\n",
        "               gamma=1.0):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.weight_layers = [0, 2, 4, 6, 7]\n",
        "\n",
        "    self.add(tf.keras.layers.Input(shape=input_shape))\n",
        "\n",
        "    self.add(conv2d_variational(6, 5, kernel_posterior_fn=kernel_posterior_fn, \n",
        "                                kernel_prior_fn=kernel_prior_fn, \n",
        "                                kernel_divergence_fn=kernel_divergence_fn))\n",
        "    self.add(max_pool())\n",
        "\n",
        "    self.add(conv2d_variational(16, 5, kernel_posterior_fn=kernel_posterior_fn, \n",
        "                                kernel_prior_fn=kernel_prior_fn, \n",
        "                                kernel_divergence_fn=kernel_divergence_fn))\n",
        "    self.add(max_pool())\n",
        "\n",
        "    self.add(conv2d_variational(120, 5, kernel_posterior_fn=kernel_posterior_fn, \n",
        "                                kernel_prior_fn=kernel_prior_fn, \n",
        "                                kernel_divergence_fn=kernel_divergence_fn))\n",
        "\n",
        "    self.add(Flatten())\n",
        "\n",
        "    self.add(dense_variational(84, tf.nn.relu, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "\n",
        "    self.add(dense_variational(nb_classes, None, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "    \n",
        "    self.add(Lambda(lambda x: x*gamma))\n",
        "     "
      ],
      "metadata": {
        "id": "USVR5Cnxmoth"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test = load_data('mnist')\n",
        "lamb=1\n",
        "model_variational = LeNet_Variational_Large(x_train[0].shape, num_classes)\n",
        "model_variational.summary()"
      ],
      "metadata": {
        "id": "9r6WjDAwXU6h",
        "outputId": "6083cb10-86c6-429c-bd83-2f4a9d32a9ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/layers/util.py:95: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  loc = add_variable_fn(\n",
            "/usr/local/lib/python3.8/dist-packages/tensorflow_probability/python/layers/util.py:105: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  untransformed_scale = add_variable_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"le_net__variational__large\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_reparameterization (  (None, 28, 28, 6)        312       \n",
            " Conv2DReparameterization)                                       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 6)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_reparameterization_1  (None, 14, 14, 16)       4832      \n",
            "  (Conv2DReparameterization)                                     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_reparameterization_2  (None, 7, 7, 120)        96240     \n",
            "  (Conv2DReparameterization)                                     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 5880)              0         \n",
            "                                                                 \n",
            " dense_reparameterization (D  (None, 84)               988008    \n",
            " enseReparameterization)                                         \n",
            "                                                                 \n",
            " dense_reparameterization_1   (None, 10)               1700      \n",
            " (DenseReparameterization)                                       \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,091,092\n",
            "Trainable params: 1,091,092\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwGNoPYQQdNE"
      },
      "source": [
        "# Training Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qXUVl6tWoO00"
      },
      "outputs": [],
      "source": [
        "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.0)\n",
        "\n",
        "@tf.function\n",
        "def ece(y_true,y_pred):\n",
        "  return tfp.stats.expected_calibration_error(10,logits=y_pred, labels_true=tf.argmax(y_true,axis=1))\n",
        "\n",
        "# Place the logs in a timestamped subdirectory\n",
        "# This allows to easy select different training runs\n",
        "# In order not to overwrite some data, it is useful to have a name with a timestamp\n",
        "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Specify the callback object\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# tf.keras.callback.TensorBoard ensures that logs are created and stored\n",
        "# We need to pass callback object to the fit method\n",
        "# The way to do this is by passing the list of callback objects, which is in our case just one\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_categorical_crossentropy\", patience=10, restore_best_weights=True)\n",
        "\n",
        "adam_opt = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "def schedule(epoch, lr):\n",
        "  if (epoch<25):\n",
        "    return 0.001\n",
        "  elif epoch<50:\n",
        "    return 0.0001\n",
        "  else:\n",
        "    return 0.00001\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "iCCKNSDXfTis"
      },
      "outputs": [],
      "source": [
        "class BMA_Model(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, model, mc_samples):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.mc_samples = mc_samples\n",
        "  \n",
        "  def call(self, inputs, training=False):\n",
        "    list_models =[self.model(inputs) for i in range(self.mc_samples)]\n",
        "    stack_models = tf.stack(list_models,axis=1)\n",
        "    return tfp.math.reduce_logmeanexp(stack_models,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPt69y9ZcusY"
      },
      "source": [
        "# Variational Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "7OWzof1fmtBb"
      },
      "outputs": [],
      "source": [
        "# experiment settings\n",
        "data_use=\"mnist\"\n",
        "model_used=\"lenet\"\n",
        "num_posterior_samples=10\n",
        "n_epochs=100\n",
        "batch_size=100\n",
        "lr=0.001\n",
        "\n",
        "seeds=[15]#,24]\n",
        "\n",
        "# lambs for likelihood tempering\n",
        "lambs=np.linspace(0.5, 3.5, 7)\n",
        "#lambs = [1.0]\n",
        "\n",
        "# label_smoothing\n",
        "# label_smoothing=0.0\n",
        "# cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=label_smoothing)\n",
        "\n",
        "# label noise\n",
        "label_noises=[0.0] #0.1,0.2,0.3]\n",
        "\n",
        "# smooth softmax\n",
        "gammas=[1.0]#,5.0,10.0]\n",
        "\n",
        "# data augmentation:\n",
        "#   - None: No data augmentation\n",
        "#   - Standard: \n",
        "#   - Replacement\n",
        "#   - Noise\n",
        "#d_augmentation = [\"Standard\", \"Replacement\", \"Noise\"]\n",
        "d_augmentation = [\"Replacement\"]\n",
        "\n",
        "# prior\n",
        "#prior_scales=[1.0, 0.1, 0.01]\n",
        "prior_scales=[1.0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "RbTGSR9jQje6"
      },
      "outputs": [],
      "source": [
        "# x_train_fashion, y_train_fashion, x_test_fashion, y_test_fashion = load_data('fashion_mnist')\n",
        "x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist = load_data('mnist')\n",
        "\n",
        "x_train_mnist = x_train_mnist[:10000]\n",
        "y_train_mnist = y_train_mnist[:10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4i8AnQ6Qt-3",
        "outputId": "af121a52-9dd9-4c17-eb00-bf7657465670"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/7 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5\n",
            "100/100 [==============================] - 2s 3ms/step - loss: 4.7210 - accuracy: 0.8844 - categorical_crossentropy: 0.8166\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7373 - accuracy: 0.8891 - categorical_crossentropy: 0.8328\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6692 - accuracy: 0.8905 - categorical_crossentropy: 0.7648\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7688 - accuracy: 0.8843 - categorical_crossentropy: 0.8644\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6896 - accuracy: 0.8862 - categorical_crossentropy: 0.7851\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7482 - accuracy: 0.8866 - categorical_crossentropy: 0.8438\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7018 - accuracy: 0.8882 - categorical_crossentropy: 0.7973\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6699 - accuracy: 0.8937 - categorical_crossentropy: 0.7655\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7000 - accuracy: 0.8870 - categorical_crossentropy: 0.7956\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6749 - accuracy: 0.8914 - categorical_crossentropy: 0.7705\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7140 - accuracy: 0.8877 - categorical_crossentropy: 0.8095\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6758 - accuracy: 0.8864 - categorical_crossentropy: 0.7714\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6628 - accuracy: 0.8901 - categorical_crossentropy: 0.7584\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7237 - accuracy: 0.8887 - categorical_crossentropy: 0.8193\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6772 - accuracy: 0.8917 - categorical_crossentropy: 0.7728\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7825 - accuracy: 0.8785 - categorical_crossentropy: 0.8781\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6700 - accuracy: 0.8909 - categorical_crossentropy: 0.7656\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7216 - accuracy: 0.8860 - categorical_crossentropy: 0.8171\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6703 - accuracy: 0.8894 - categorical_crossentropy: 0.7659\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7118 - accuracy: 0.8885 - categorical_crossentropy: 0.8074\n",
            "100/100 [==============================] - 14s 7ms/step - loss: 39.0444 - accuracy: 0.9281 - categorical_crossentropy: 0.5142\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 39.0444 - accuracy: 0.9287 - categorical_crossentropy: 0.5226\n",
            "Tue Apr 18 15:18:47 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   70C    P0    39W /  70W |   9015MiB / 15360MiB |     71%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 1/7 [01:16<07:36, 76.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "100/100 [==============================] - 2s 3ms/step - loss: 5.3265 - accuracy: 0.9083 - categorical_crossentropy: 1.2376\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3359 - accuracy: 0.9069 - categorical_crossentropy: 1.2470\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2479 - accuracy: 0.9092 - categorical_crossentropy: 1.1590\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2830 - accuracy: 0.9101 - categorical_crossentropy: 1.1940\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2053 - accuracy: 0.9060 - categorical_crossentropy: 1.1164\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2800 - accuracy: 0.9026 - categorical_crossentropy: 1.1910\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2898 - accuracy: 0.9051 - categorical_crossentropy: 1.2008\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2994 - accuracy: 0.9064 - categorical_crossentropy: 1.2104\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2523 - accuracy: 0.9112 - categorical_crossentropy: 1.1633\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2324 - accuracy: 0.9093 - categorical_crossentropy: 1.1434\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2674 - accuracy: 0.9082 - categorical_crossentropy: 1.1784\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3160 - accuracy: 0.9021 - categorical_crossentropy: 1.2270\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1355 - accuracy: 0.9126 - categorical_crossentropy: 1.0465\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2760 - accuracy: 0.9101 - categorical_crossentropy: 1.1870\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2383 - accuracy: 0.9069 - categorical_crossentropy: 1.1493\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3227 - accuracy: 0.9055 - categorical_crossentropy: 1.2337\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1973 - accuracy: 0.9094 - categorical_crossentropy: 1.1084\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2267 - accuracy: 0.9103 - categorical_crossentropy: 1.1377\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1778 - accuracy: 0.9099 - categorical_crossentropy: 1.0889\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3043 - accuracy: 0.9056 - categorical_crossentropy: 1.2153\n",
            "100/100 [==============================] - 14s 7ms/step - loss: 40.8898 - accuracy: 0.9351 - categorical_crossentropy: 0.8140\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 40.8898 - accuracy: 0.9360 - categorical_crossentropy: 0.8563\n",
            "Tue Apr 18 15:20:02 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    40W /  70W |   9015MiB / 15360MiB |     67%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▊       | 2/7 [02:30<06:16, 75.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5\n",
            "100/100 [==============================] - 2s 3ms/step - loss: 5.1772 - accuracy: 0.9213 - categorical_crossentropy: 1.2799\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2462 - accuracy: 0.9193 - categorical_crossentropy: 1.3489\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1131 - accuracy: 0.9213 - categorical_crossentropy: 1.2158\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1680 - accuracy: 0.9193 - categorical_crossentropy: 1.2708\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1455 - accuracy: 0.9161 - categorical_crossentropy: 1.2482\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1608 - accuracy: 0.9201 - categorical_crossentropy: 1.2635\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1628 - accuracy: 0.9220 - categorical_crossentropy: 1.2655\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2217 - accuracy: 0.9211 - categorical_crossentropy: 1.3244\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.0903 - accuracy: 0.9223 - categorical_crossentropy: 1.1930\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1244 - accuracy: 0.9236 - categorical_crossentropy: 1.2272\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1635 - accuracy: 0.9212 - categorical_crossentropy: 1.2662\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1928 - accuracy: 0.9199 - categorical_crossentropy: 1.2955\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.0788 - accuracy: 0.9201 - categorical_crossentropy: 1.1815\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1819 - accuracy: 0.9189 - categorical_crossentropy: 1.2846\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1064 - accuracy: 0.9191 - categorical_crossentropy: 1.2091\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2494 - accuracy: 0.9146 - categorical_crossentropy: 1.3521\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1548 - accuracy: 0.9187 - categorical_crossentropy: 1.2576\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1756 - accuracy: 0.9180 - categorical_crossentropy: 1.2783\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1259 - accuracy: 0.9207 - categorical_crossentropy: 1.2287\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1768 - accuracy: 0.9156 - categorical_crossentropy: 1.2796\n",
            "100/100 [==============================] - 14s 7ms/step - loss: 38.9727 - accuracy: 0.9428 - categorical_crossentropy: 0.8689\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 38.9727 - accuracy: 0.9434 - categorical_crossentropy: 0.9192\n",
            "Tue Apr 18 15:21:16 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    39W /  70W |   9017MiB / 15360MiB |     53%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 43%|████▎     | 3/7 [03:45<04:59, 74.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0\n",
            "100/100 [==============================] - 2s 3ms/step - loss: 5.3437 - accuracy: 0.9233 - categorical_crossentropy: 1.6133\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.4343 - accuracy: 0.9200 - categorical_crossentropy: 1.7040\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3030 - accuracy: 0.9255 - categorical_crossentropy: 1.5727\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3428 - accuracy: 0.9230 - categorical_crossentropy: 1.6124\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3295 - accuracy: 0.9184 - categorical_crossentropy: 1.5991\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3160 - accuracy: 0.9243 - categorical_crossentropy: 1.5856\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3684 - accuracy: 0.9240 - categorical_crossentropy: 1.6380\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3406 - accuracy: 0.9255 - categorical_crossentropy: 1.6102\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2787 - accuracy: 0.9256 - categorical_crossentropy: 1.5483\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3683 - accuracy: 0.9212 - categorical_crossentropy: 1.6379\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3542 - accuracy: 0.9246 - categorical_crossentropy: 1.6238\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3735 - accuracy: 0.9197 - categorical_crossentropy: 1.6432\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2203 - accuracy: 0.9246 - categorical_crossentropy: 1.4899\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3516 - accuracy: 0.9210 - categorical_crossentropy: 1.6212\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2590 - accuracy: 0.9213 - categorical_crossentropy: 1.5287\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.4993 - accuracy: 0.9201 - categorical_crossentropy: 1.7689\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3520 - accuracy: 0.9227 - categorical_crossentropy: 1.6217\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3421 - accuracy: 0.9199 - categorical_crossentropy: 1.6117\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2325 - accuracy: 0.9237 - categorical_crossentropy: 1.5021\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3358 - accuracy: 0.9218 - categorical_crossentropy: 1.6054\n",
            "100/100 [==============================] - 14s 7ms/step - loss: 37.3038 - accuracy: 0.9451 - categorical_crossentropy: 1.1317\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 37.3038 - accuracy: 0.9435 - categorical_crossentropy: 1.1372\n",
            "Tue Apr 18 15:22:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    40W /  70W |   9017MiB / 15360MiB |     47%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 57%|█████▋    | 4/7 [04:58<03:42, 74.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5\n",
            "100/100 [==============================] - 2s 3ms/step - loss: 5.3473 - accuracy: 0.9190 - categorical_crossentropy: 1.8669\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.4151 - accuracy: 0.9200 - categorical_crossentropy: 1.9346\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2069 - accuracy: 0.9231 - categorical_crossentropy: 1.7265\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2377 - accuracy: 0.9225 - categorical_crossentropy: 1.7573\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2342 - accuracy: 0.9194 - categorical_crossentropy: 1.7537\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2606 - accuracy: 0.9230 - categorical_crossentropy: 1.7802\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2799 - accuracy: 0.9210 - categorical_crossentropy: 1.7995\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3110 - accuracy: 0.9224 - categorical_crossentropy: 1.8306\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2185 - accuracy: 0.9250 - categorical_crossentropy: 1.7380\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2648 - accuracy: 0.9224 - categorical_crossentropy: 1.7844\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3060 - accuracy: 0.9231 - categorical_crossentropy: 1.8256\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2694 - accuracy: 0.9199 - categorical_crossentropy: 1.7890\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.1838 - accuracy: 0.9208 - categorical_crossentropy: 1.7034\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2184 - accuracy: 0.9226 - categorical_crossentropy: 1.7380\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2036 - accuracy: 0.9232 - categorical_crossentropy: 1.7232\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2853 - accuracy: 0.9212 - categorical_crossentropy: 1.8048\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2322 - accuracy: 0.9200 - categorical_crossentropy: 1.7518\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2384 - accuracy: 0.9228 - categorical_crossentropy: 1.7580\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.2225 - accuracy: 0.9228 - categorical_crossentropy: 1.7421\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 5.3977 - accuracy: 0.9186 - categorical_crossentropy: 1.9173\n",
            "100/100 [==============================] - 14s 7ms/step - loss: 34.8041 - accuracy: 0.9451 - categorical_crossentropy: 1.2981\n",
            "100/100 [==============================] - 1s 7ms/step - loss: 34.8041 - accuracy: 0.9421 - categorical_crossentropy: 1.3075\n",
            "Tue Apr 18 15:23:45 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    39W /  70W |   9019MiB / 15360MiB |     70%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████▏  | 5/7 [06:14<02:29, 74.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.0\n",
            "100/100 [==============================] - 2s 3ms/step - loss: 4.8723 - accuracy: 0.9196 - categorical_crossentropy: 1.5523\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.9821 - accuracy: 0.9173 - categorical_crossentropy: 1.6621\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.8495 - accuracy: 0.9197 - categorical_crossentropy: 1.5295\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.9265 - accuracy: 0.9183 - categorical_crossentropy: 1.6065\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.8612 - accuracy: 0.9171 - categorical_crossentropy: 1.5411\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.8700 - accuracy: 0.9178 - categorical_crossentropy: 1.5499\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.9445 - accuracy: 0.9177 - categorical_crossentropy: 1.6245\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.8366 - accuracy: 0.9237 - categorical_crossentropy: 1.5166\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.8323 - accuracy: 0.9205 - categorical_crossentropy: 1.5122\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.8513 - accuracy: 0.9170 - categorical_crossentropy: 1.5312\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.9144 - accuracy: 0.9192 - categorical_crossentropy: 1.5943\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.9381 - accuracy: 0.9183 - categorical_crossentropy: 1.6180\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.8579 - accuracy: 0.9183 - categorical_crossentropy: 1.5379\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.9189 - accuracy: 0.9167 - categorical_crossentropy: 1.5988\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.8798 - accuracy: 0.9196 - categorical_crossentropy: 1.5597\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.9486 - accuracy: 0.9205 - categorical_crossentropy: 1.6286\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.8933 - accuracy: 0.9193 - categorical_crossentropy: 1.5733\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.8790 - accuracy: 0.9191 - categorical_crossentropy: 1.5589\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.8564 - accuracy: 0.9178 - categorical_crossentropy: 1.5363\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.9296 - accuracy: 0.9180 - categorical_crossentropy: 1.6096\n",
            "100/100 [==============================] - 14s 7ms/step - loss: 33.2006 - accuracy: 0.9413 - categorical_crossentropy: 1.1775\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 33.2006 - accuracy: 0.9394 - categorical_crossentropy: 1.1518\n",
            "Tue Apr 18 15:24:59 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    39W /  70W |   9019MiB / 15360MiB |     72%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 86%|████████▌ | 6/7 [07:28<01:14, 74.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5\n",
            "100/100 [==============================] - 2s 3ms/step - loss: 4.8026 - accuracy: 0.9239 - categorical_crossentropy: 1.7033\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7553 - accuracy: 0.9230 - categorical_crossentropy: 1.6560\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6841 - accuracy: 0.9225 - categorical_crossentropy: 1.5848\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6496 - accuracy: 0.9266 - categorical_crossentropy: 1.5504\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6846 - accuracy: 0.9213 - categorical_crossentropy: 1.5854\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.5961 - accuracy: 0.9244 - categorical_crossentropy: 1.4969\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7017 - accuracy: 0.9249 - categorical_crossentropy: 1.6025\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6044 - accuracy: 0.9283 - categorical_crossentropy: 1.5052\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6750 - accuracy: 0.9223 - categorical_crossentropy: 1.5757\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6371 - accuracy: 0.9271 - categorical_crossentropy: 1.5379\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7377 - accuracy: 0.9230 - categorical_crossentropy: 1.6384\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.5900 - accuracy: 0.9273 - categorical_crossentropy: 1.4907\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6393 - accuracy: 0.9232 - categorical_crossentropy: 1.5400\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6706 - accuracy: 0.9217 - categorical_crossentropy: 1.5714\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7187 - accuracy: 0.9199 - categorical_crossentropy: 1.6195\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7819 - accuracy: 0.9187 - categorical_crossentropy: 1.6827\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.7276 - accuracy: 0.9267 - categorical_crossentropy: 1.6283\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6682 - accuracy: 0.9242 - categorical_crossentropy: 1.5690\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6272 - accuracy: 0.9248 - categorical_crossentropy: 1.5279\n",
            "100/100 [==============================] - 0s 3ms/step - loss: 4.6748 - accuracy: 0.9232 - categorical_crossentropy: 1.5756\n",
            "100/100 [==============================] - 14s 7ms/step - loss: 30.9924 - accuracy: 0.9405 - categorical_crossentropy: 1.2807\n",
            "100/100 [==============================] - 1s 6ms/step - loss: 30.9924 - accuracy: 0.9413 - categorical_crossentropy: 1.1306\n",
            "Tue Apr 18 15:26:12 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    39W /  70W |   9021MiB / 15360MiB |     69%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7/7 [08:41<00:00, 74.49s/it]\n"
          ]
        }
      ],
      "source": [
        "model_variational=[]\n",
        "for prior_scale in prior_scales: \n",
        "  for gamma in gammas:\n",
        "    for label_noise in label_noises:\n",
        "      for da in d_augmentation:\n",
        "\n",
        "        setting_name=f\"prior_scale_{prior_scale}_label_noise_{label_noise}_smooth_softmax_{gamma}_data_augmentation_{da}\" \n",
        "\n",
        "        x_train = x_train_mnist\n",
        "        y_train = y_train_mnist\n",
        "        x_test = x_test_mnist\n",
        "        y_test = y_test_mnist\n",
        "\n",
        "        input_shape=x_train[0].shape\n",
        "\n",
        "        if label_noise > 0.0:\n",
        "          import random\n",
        "          # train\n",
        "          ind_noisy=random.sample(range(1, y_train.shape[0]), int(y_train.shape[0]*label_noise))\n",
        "          for ind in ind_noisy:\n",
        "            false_label=random.sample(range(1,y_train.shape[1]),1)[0]\n",
        "            while y_train[ind, false_label] != 1.0:\n",
        "              temp=np.zeros((y_train.shape[1]))\n",
        "              temp[false_label]=1.0\n",
        "              y_train[ind, :]=temp\n",
        "              break\n",
        "            else:\n",
        "              false_label=random.sample(range(1,y_train.shape[1]),1)[0]\n",
        "          # test\n",
        "          ind_noisy=random.sample(range(1, y_test.shape[0]), int(y_test.shape[0]*label_noise))\n",
        "          for ind in ind_noisy:\n",
        "            false_label=random.sample(range(1,y_test.shape[1]),1)[0]\n",
        "            while y_test[ind, false_label] != 1.0:\n",
        "              temp=np.zeros((y_test.shape[1]))\n",
        "              temp[false_label]=1.0\n",
        "              y_test[ind, :]=temp\n",
        "              break\n",
        "            else:\n",
        "              false_label=random.sample(range(1,y_test.shape[1]),1)[0]\n",
        "\n",
        "        from tensorflow import keras\n",
        "        from tensorflow.keras import layers\n",
        "\n",
        "        if da == \"None\":\n",
        "\n",
        "            train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "            train_loader = train_loader.batch(batch_size)\n",
        "\n",
        "        if da == \"Standard\":\n",
        "            # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
        "            data_augmentation = keras.Sequential(\n",
        "                [\n",
        "                    layers.RandomFlip(\"horizontal\"),\n",
        "                    layers.RandomRotation(0.1),\n",
        "                    layers.RandomZoom(0.1),\n",
        "                ]\n",
        "            )\n",
        "            # Create a tf.data pipeline of augmented images (and their labels)\n",
        "            train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "            train_loader = train_loader.batch(batch_size).map(lambda x, y: (data_augmentation(x), y))\n",
        "\n",
        "\n",
        "        if da == \"Noise\":\n",
        "\n",
        "            rng = np.random.default_rng(1234)\n",
        "            n_features = x_train.shape[1]*x_train.shape[2]*x_train.shape[3]\n",
        "            idx = np.random.choice(np.arange(0, n_features, 1), n_features // 5 * 4, replace = False)\n",
        "            perm = rng.permutation(idx)\n",
        "\n",
        "            aux =  x_train.reshape(x_train.shape[0], -1)\n",
        "            aux[:, idx] = aux[:, perm]\n",
        "\n",
        "            x_train = aux.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
        "\n",
        "            # Create a tf.data pipeline of augmented images (and their labels)\n",
        "            train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "            train_loader = train_loader.batch(batch_size)\n",
        "\n",
        "        if da == \"Replacement\":\n",
        "\n",
        "            rng = np.random.default_rng(1234)\n",
        "\n",
        "            idx = np.random.choice(np.arange(0, x_train.shape[0], 1), int(x_train.shape[0]*0.05), replace= False)\n",
        "            idx = np.random.choice(idx, x_train.shape[0], replace= True)\n",
        "\n",
        "            # Create a tf.data pipeline of augmented images (and their labels)\n",
        "            train_loader = tf.data.Dataset.from_tensor_slices((x_train[idx], y_train[idx]))\n",
        "            train_loader = train_loader.batch(batch_size)\n",
        "        \n",
        "        \n",
        "        input_shape = x_train.shape[1:]\n",
        "        results={}\n",
        "\n",
        "        for seed in seeds:\n",
        "          print(seed)\n",
        "          results[seed]={} # a dict, with the structure, dict_results[lamb]=[log_ps_train, log_ps_test, metrics_bma]\n",
        "          for lamb in tqdm(lambs):\n",
        "            \n",
        "            print(lamb)\n",
        "            results[seed][lamb]=[]\n",
        "\n",
        "            tf.keras.utils.set_random_seed(seed)\n",
        "\n",
        "            model_variational = LeNet_Variational_Large(input_shape, num_classes,\n",
        "                                                      kernel_prior_fn=make_normal_prior(prior_scale),\n",
        "                                                      kernel_divergence_fn=lambda q, p, ignore: kl_lib.kl_divergence(q, p)/(x_train.shape[0]*lamb),\n",
        "                                                      gamma=gamma)\n",
        "\n",
        "            model_variational.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
        "                            loss = cce,\n",
        "                            metrics=['accuracy', cce]\n",
        "                            )\n",
        "\n",
        "\n",
        "            model_variational.fit(train_loader, \n",
        "                    epochs=n_epochs, \n",
        "                    #validation_data = (x_test, y_test),\n",
        "                    # workers=8,\n",
        "                    verbose = 0)\n",
        "            \n",
        "\n",
        "            log_ps_train=[]\n",
        "            log_ps_test=[]\n",
        "            metrics_gibbs_train=[]\n",
        "            metrics_gibbs_test=[]\n",
        "            metrics_bayes_train=[]\n",
        "            metrics_bayes_test=[]\n",
        "\n",
        "            # loop through posterior samples\n",
        "            for i in range(num_posterior_samples):\n",
        "              \n",
        "              # compute the categorical dist. and log_p for train data\n",
        "              x_train_augmented = np.concatenate([x for x, y in train_loader.as_numpy_iterator()])\n",
        "              y_train_augmented = np.concatenate([y for x, y in train_loader.as_numpy_iterator()])\n",
        "              p_train=tfp.distributions.Categorical(logits=model_variational(x_train_augmented))\n",
        "              log_p_train=p_train.log_prob(tf.argmax(y_train_augmented, axis=1))\n",
        "\n",
        "              log_ps_train.append(log_p_train)\n",
        "\n",
        "              # compute the categorical dist. and log_p for test data\n",
        "              p_test=tfp.distributions.Categorical(logits=model_variational(x_test)) \n",
        "              log_p_test=p_test.log_prob(tf.argmax(y_test, axis=1))\n",
        "              log_ps_test.append(log_p_test)\n",
        "\n",
        "\n",
        "\n",
        "              # compute gibbs-based metrics\n",
        "              metrics_gibbs_train.append(model_variational.evaluate(x_train, y_train, batch_size=batch_size))\n",
        "              metrics_gibbs_test.append(model_variational.evaluate(x_test, y_test, batch_size=batch_size))\n",
        "\n",
        "            # compute bayes-based metric\n",
        "            bma_model=BMA_Model(model_variational, num_posterior_samples)\n",
        "            bma_model.compile(metrics=['accuracy', cce])\n",
        "            metrics_bayes_train.append(bma_model.evaluate(x_train, y_train, batch_size=batch_size))\n",
        "            metrics_bayes_test.append(bma_model.evaluate(x_test, y_test, batch_size=batch_size))\n",
        "\n",
        "            !nvidia-smi\n",
        "            \n",
        "            # save log_p for train data\n",
        "            results[seed][lamb].append(tf.stack(log_ps_train,axis=1).numpy())\n",
        "            # save log_p for test data \n",
        "            results[seed][lamb].append(tf.stack(log_ps_test,axis=1).numpy())\n",
        "            # save gibbs-based metric\n",
        "            results[seed][lamb].append(metrics_gibbs_train)\n",
        "            results[seed][lamb].append(metrics_gibbs_test)\n",
        "            # save bayes-based metric\n",
        "            results[seed][lamb].append(metrics_bayes_train)\n",
        "            results[seed][lamb].append(metrics_bayes_test)\n",
        "\n",
        "        with open(f'{setting_name}.pickle', 'wb') as handle:\n",
        "          pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "MjOWNM9Cla0u"
      },
      "outputs": [],
      "source": [
        "# if if_da:\n",
        "#   from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#   datagen = ImageDataGenerator(\n",
        "#           rotation_range=10,  # Rotate the image randomly by up to 10 degrees\n",
        "#           zoom_range=0.1,     # Zoom in or out of the image randomly by up to 10%\n",
        "#           width_shift_range=0.1,  # Shift the image horizontally by up to 10%\n",
        "#           height_shift_range=0.1, # Shift the image vertically by up to 10%\n",
        "#           )\n",
        "#   datagen.fit(x_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "TB60tuuQzZ3q"
      },
      "outputs": [],
      "source": [
        "# if if_da:\n",
        "#   from tensorflow import keras\n",
        "#   from tensorflow.keras import layers\n",
        "\n",
        "#   # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
        "#   data_augmentation = keras.Sequential(\n",
        "#       [\n",
        "#           layers.RandomFlip(\"horizontal\"),\n",
        "#           layers.RandomRotation(0.1),\n",
        "#           layers.RandomZoom(0.1),\n",
        "#       ]\n",
        "#   )\n",
        "\n",
        "#   input_shape = x_train.shape[1:]\n",
        "#   classes = 10\n",
        "\n",
        "#   # Create a tf.data pipeline of augmented images (and their labels)\n",
        "#   train_augmented = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "#   train_augmented = train_augmented.batch(batch_size).map(lambda x, y: (data_augmentation(x), y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_variational.summary()"
      ],
      "metadata": {
        "id": "hgXENp57XfDs",
        "outputId": "6c82c82a-7702-4314-9c5f-224cf38e7939",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"le_net__variational__large_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_reparameterization_9  (None, 28, 28, 6)        312       \n",
            " 0 (Conv2DReparameterization                                     \n",
            " )                                                               \n",
            "                                                                 \n",
            " max_pooling2d_60 (MaxPoolin  (None, 14, 14, 6)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_reparameterization_9  (None, 14, 14, 16)       4832      \n",
            " 1 (Conv2DReparameterization                                     \n",
            " )                                                               \n",
            "                                                                 \n",
            " max_pooling2d_61 (MaxPoolin  (None, 7, 7, 16)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_reparameterization_9  (None, 7, 7, 120)        96240     \n",
            " 2 (Conv2DReparameterization                                     \n",
            " )                                                               \n",
            "                                                                 \n",
            " flatten_30 (Flatten)        (None, 5880)              0         \n",
            "                                                                 \n",
            " dense_reparameterization_60  (None, 84)               988008    \n",
            "  (DenseReparameterization)                                      \n",
            "                                                                 \n",
            " dense_reparameterization_61  (None, 10)               1700      \n",
            "  (DenseReparameterization)                                      \n",
            "                                                                 \n",
            " lambda_30 (Lambda)          (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,091,092\n",
            "Trainable params: 1,091,092\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "7bAlp9m5Zg9q"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "KJEY_ldvaQ83"
      },
      "outputs": [],
      "source": [
        "# files.download('prior_scale_1.0_label_noise_0.0_smooth_softmax_3.0_data_augmentation_False.pickle') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "itHnSRJmgOXp"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "sqmjiCxDXxUO"
      },
      "outputs": [],
      "source": [
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1vfKZAloS2gJ"
      },
      "execution_count": 64,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}