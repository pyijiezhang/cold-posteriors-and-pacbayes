{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pyijiezhang/cold-posteriors-and-pacbayes/blob/main/Yijie's_Variational_NNs_Keras_TFP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2EOnCShVSJ4"
      },
      "source": [
        "# Set-Up\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow -y"
      ],
      "metadata": {
        "id": "4siUQ4-eU0qC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a3713f2-8681-44ee-eed9-212d604270d2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: tensorflow 2.8.3\n",
            "Uninstalling tensorflow-2.8.3:\n",
            "  Successfully uninstalled tensorflow-2.8.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xBfZZK217Xpp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2986a39-4cf6-4262-e00d-dce11de30ba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8.3\n",
            "  Using cached tensorflow-2.8.3-cp39-cp39-manylinux2010_x86_64.whl (498.5 MB)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (4.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (67.6.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (23.3.3)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (0.32.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (2.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.53.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.14.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (3.19.6)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (1.22.4)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (16.0.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (3.8.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.8.3) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow==2.8.3) (0.40.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.27.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.2.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.17.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8.3) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (6.2.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=0.11.15->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8.3) (3.2.2)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-2.8.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.8.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall import tensorflow_probability -y"
      ],
      "metadata": {
        "id": "UYOX3i2JVG-a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ded692-9de9-4ef5-a5cb-08944a461874"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping import as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: tensorflow-probability 0.16.0\n",
            "Uninstalling tensorflow-probability-0.16.0:\n",
            "  Successfully uninstalled tensorflow-probability-0.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u0o1OfUk9BQh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b81795-adca-4fb1-881a-f8cbeae74603"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow_probability==0.16.0\n",
            "  Using cached tensorflow_probability-0.16.0-py2.py3-none-any.whl (6.3 MB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability==0.16.0) (1.4.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability==0.16.0) (0.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability==0.16.0) (4.4.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability==0.16.0) (1.16.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability==0.16.0) (2.2.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability==0.16.0) (1.22.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.9/dist-packages (from tensorflow_probability==0.16.0) (0.1.8)\n",
            "Installing collected packages: tensorflow_probability\n",
            "Successfully installed tensorflow_probability-0.16.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow_probability==0.16.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiLjNLa2Fxln",
        "outputId": "818a44c5-2393-479a-d915-579ee4b9a7a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kFbyHYemoTLm"
      },
      "outputs": [],
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lXsO6OyvFiJz"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, AveragePooling2D, Lambda\n",
        "\n",
        "from tensorflow.keras import datasets\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ezfO-JPQ9JNs"
      },
      "outputs": [],
      "source": [
        "assert(tf.test.gpu_device_name())\n",
        "tf.keras.backend.clear_session()\n",
        "tf.config.optimizer.set_jit(True) # Enable XLA."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pickle"
      ],
      "metadata": {
        "id": "rvBgUpdYhcTe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhtuinGb7p7X",
        "outputId": "2fedc43c-c1bd-4fe7-b6cb-421cd2eeec30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.3\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyjb_iYxcdCR"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SXIhRtCfxCFB"
      },
      "outputs": [],
      "source": [
        "num_classes = 10\n",
        "\n",
        "def load_data(data_set):\n",
        "  if data_set=='fashion_mnist':\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.fashion_mnist.load_data()\n",
        "  else:\n",
        "    (x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()\n",
        "  \n",
        "\n",
        "  tf.keras.utils.set_random_seed(123)\n",
        "  a = np.random.permutation(60000) #60000 original\n",
        "  x_train = x_train[a,...]\n",
        "  y_train = y_train[a]\n",
        "  # Add a new axis\n",
        "  x_train = x_train[..., np.newaxis]\n",
        "  x_test = x_test[..., np.newaxis]\n",
        "\n",
        "\n",
        "  # Convert class vectors to binary class matrices.\n",
        "  y_train = to_categorical(y_train, num_classes)\n",
        "  y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "  # Data normalization\n",
        "  x_train = x_train.astype('float32')\n",
        "  x_test = x_test.astype('float32')\n",
        "  x_train /= 255\n",
        "  x_test /= 255\n",
        "\n",
        "  return x_train, y_train, x_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdc9t3j5os2T"
      },
      "source": [
        "# Prior Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi3i0Qya01CV"
      },
      "source": [
        "Define functions to initialize different types of prior distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1P33CQe3MYeo"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.python.distributions import normal as normal_lib\n",
        "from tensorflow_probability.python.distributions import independent as independent_lib\n",
        "from tensorflow_probability.python.distributions import kullback_leibler as kl_lib\n",
        "\n",
        "def make_normal_prior(scale):\n",
        "  \"\"\"\n",
        "  Defines a function that returns a Gaussian prior with the given standard\n",
        "  deviation.\n",
        "  \"\"\"\n",
        "  def _fn(dtype, shape, name, trainable,add_variable_fn):\n",
        "    del name, trainable, add_variable_fn   # unused\n",
        "    dist = normal_lib.Normal(loc=tf.zeros(shape, dtype), \n",
        "                             scale=dtype.as_numpy_dtype(scale))\n",
        "    batch_ndims = tf.size(dist.batch_shape_tensor())\n",
        "    return independent_lib.Independent(dist, \n",
        "                                       reinterpreted_batch_ndims=batch_ndims)\n",
        "\n",
        "  return _fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9E9K-6kuPq0"
      },
      "source": [
        "# Posteriors Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6FwtaZJ3EYu"
      },
      "source": [
        "Define posterior distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QoVqrmiDi5r1"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf1\n",
        "\n",
        "mean_field_init_untransformed_scale = -7.0\n",
        "\n",
        "# TODO(trandustin): Remove need for this boilerplate code.\n",
        "def mean_field_fn(empirical_bayes=False,\n",
        "                  initializer=tf1.initializers.he_normal()):\n",
        "  \"\"\"Constructors for Gaussian prior and posterior distributions.\n",
        "  Args:\n",
        "    empirical_bayes (bool): Whether to train the variance of the prior or not.\n",
        "    initializer (tf1.initializer): Initializer for the posterior means.\n",
        "  Returns:\n",
        "    prior, posterior (tfp.distribution): prior and posterior\n",
        "    to be fed into a Bayesian Layer.\n",
        "  \"\"\"\n",
        "\n",
        "  def prior(dtype, shape, name, trainable, add_variable_fn):\n",
        "    \"\"\"Returns the prior distribution (tfp.distributions.Independent).\"\"\"\n",
        "    softplus_inverse_scale = np.log(np.exp(1.) - 1.)\n",
        "\n",
        "    istrainable = add_variable_fn(\n",
        "        name=name + '_istrainable',\n",
        "        shape=(),\n",
        "        initializer=tf1.constant_initializer(1.),\n",
        "        dtype=dtype,\n",
        "        trainable=False)\n",
        "\n",
        "    untransformed_scale = add_variable_fn(\n",
        "        name=name + '_untransformed_scale',\n",
        "        shape=(),\n",
        "        initializer=tf1.constant_initializer(softplus_inverse_scale),\n",
        "        dtype=dtype,\n",
        "        trainable=empirical_bayes and trainable)\n",
        "    scale = (\n",
        "        np.finfo(dtype.as_numpy_dtype).eps +\n",
        "        tf.nn.softplus(untransformed_scale * istrainable + (1. - istrainable) *\n",
        "                       tf1.stop_gradient(untransformed_scale)))\n",
        "    loc = add_variable_fn(\n",
        "        name=name + '_loc',\n",
        "        shape=shape,\n",
        "        initializer=tf1.constant_initializer(0.),\n",
        "        dtype=dtype,\n",
        "        trainable=False)\n",
        "    dist = tfp.distributions.Normal(loc=loc, scale=scale)\n",
        "    dist.istrainable = istrainable\n",
        "    dist.untransformed_scale = untransformed_scale\n",
        "    batch_ndims = tf1.size(input=dist.batch_shape_tensor())\n",
        "    return tfp.distributions.Independent(dist,\n",
        "                                         reinterpreted_batch_ndims=batch_ndims)\n",
        "\n",
        "  def posterior(dtype, shape, name, trainable, add_variable_fn):\n",
        "    \"\"\"Returns the posterior distribution (tfp.distributions.Independent).\"\"\"\n",
        "    untransformed_scale = add_variable_fn(\n",
        "        name=name + '_untransformed_scale',\n",
        "        shape=shape,\n",
        "        initializer=tf1.initializers.random_normal(\n",
        "            mean=mean_field_init_untransformed_scale, stddev=0.1),\n",
        "        dtype=dtype,\n",
        "        trainable=trainable)\n",
        "    scale = (\n",
        "        np.finfo(dtype.as_numpy_dtype).eps +\n",
        "        tf.nn.softplus(untransformed_scale))\n",
        "    loc = add_variable_fn(\n",
        "        name=name + '_loc',\n",
        "        shape=shape,\n",
        "        initializer=initializer,\n",
        "        dtype=dtype,\n",
        "        trainable=trainable)\n",
        "    dist = tfp.distributions.Normal(loc=loc, scale=scale)\n",
        "    dist.untransformed_scale = untransformed_scale\n",
        "    batch_ndims = tf1.size(input=dist.batch_shape_tensor())\n",
        "    return tfp.distributions.Independent(dist,\n",
        "                                         reinterpreted_batch_ndims=batch_ndims)\n",
        "\n",
        "  return prior, posterior\n",
        "\n",
        "prior_fn, posterior_fn = mean_field_fn(empirical_bayes=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uKuT191d3Xzh"
      },
      "outputs": [],
      "source": [
        "from tensorflow_probability.python.distributions import deterministic as deterministic_lib\n",
        "\n",
        "def make_normal_posterior(\n",
        "    is_singular=False,\n",
        "    loc_initializer=tf1.initializers.random_normal(stddev=0.1),\n",
        "    untransformed_scale_initializer=tf1.initializers.random_normal(\n",
        "        mean=-3., stddev=0.1),\n",
        "    loc_regularizer=None,\n",
        "    untransformed_scale_regularizer=None,\n",
        "    loc_constraint=None,\n",
        "    untransformed_scale_constraint=None):\n",
        "\n",
        "  loc_scale_fn = tfp.layers.util.default_loc_scale_fn(\n",
        "      is_singular=is_singular,\n",
        "      loc_initializer=loc_initializer,\n",
        "      untransformed_scale_initializer=untransformed_scale_initializer,\n",
        "      loc_regularizer=loc_regularizer,\n",
        "      untransformed_scale_regularizer=untransformed_scale_regularizer,\n",
        "      loc_constraint=loc_constraint,\n",
        "      untransformed_scale_constraint=untransformed_scale_constraint)\n",
        "  def _fn(dtype, shape, name, trainable, add_variable_fn):\n",
        "    loc, scale = loc_scale_fn(dtype, shape, name, trainable, add_variable_fn)\n",
        "    if scale is None:\n",
        "      dist = deterministic_lib.Deterministic(loc=loc)\n",
        "    else:\n",
        "      dist = normal_lib.Normal(loc=loc, scale=scale)\n",
        "    batch_ndims = tf.size(dist.batch_shape_tensor())\n",
        "    return independent_lib.Independent(\n",
        "        dist, reinterpreted_batch_ndims=batch_ndims)\n",
        "  return _fn\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BW9rqXi-QlKc"
      },
      "outputs": [],
      "source": [
        "import tensorflow.compat.v1 as tf1\n",
        "from tensorflow_probability.python.distributions import deterministic as deterministic_lib\n",
        "\n",
        "def default_mean_field_mixture_fn(\n",
        "    is_singular=False,\n",
        "    loc_initializer=tf1.initializers.random_normal(stddev=0.1),\n",
        "    untransformed_scale_initializer=tf1.initializers.random_normal(\n",
        "        mean=-3., stddev=0.1),\n",
        "    loc_regularizer=None,\n",
        "    untransformed_scale_regularizer=None,\n",
        "    loc_constraint=None,\n",
        "    untransformed_scale_constraint=None):\n",
        " \n",
        "  loc_scale_fn = tfp.layers.util.default_loc_scale_fn(\n",
        "      is_singular=is_singular,\n",
        "      loc_initializer=loc_initializer,\n",
        "      untransformed_scale_initializer=untransformed_scale_initializer,\n",
        "      loc_regularizer=loc_regularizer,\n",
        "      untransformed_scale_regularizer=untransformed_scale_regularizer,\n",
        "      loc_constraint=loc_constraint,\n",
        "      untransformed_scale_constraint=untransformed_scale_constraint)\n",
        "  def _fn(dtype, shape, name, trainable, add_variable_fn):\n",
        "    loc, scale = loc_scale_fn(dtype, shape, name, trainable, add_variable_fn)\n",
        "    #loc_2, scale_2 = loc_scale_fn(dtype, shape, name, trainable, add_variable_fn)\n",
        "    if scale is None:\n",
        "      dist = deterministic_lib.Deterministic(loc=loc)\n",
        "    else:\n",
        "      dist = tfp.distributions.Normal(loc=loc, scale=scale)\n",
        "      #dist2 = tfp.distributions.Normal(loc=tf.stop_gradient(loc_2), scale=scale_2)\n",
        "\n",
        "    batch_ndims = tf.size(dist.batch_shape_tensor())\n",
        "\n",
        "    return tfp.distributions.Mixture(\n",
        "        cat=tfp.distributions.Categorical(probs=[0.5, 0.5]),\n",
        "        components=[\n",
        "            tfp.distributions.Independent(dist,\n",
        "                            reinterpreted_batch_ndims=batch_ndims),\n",
        "            tfp.distributions.Independent(tfp.distributions.Normal(\n",
        "                loc=tf.zeros(shape, dtype=dtype), \n",
        "                scale=1.0*tf.ones(shape, dtype=dtype)),\n",
        "                            reinterpreted_batch_ndims=batch_ndims)],\n",
        "        name='spike_and_slab')\n",
        "\n",
        "  return _fn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR-MGa04cmVh"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJyqZswKTPil"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VMA-Cx8KULte"
      },
      "outputs": [],
      "source": [
        "def conv2d(filters, kernel_size, padding='SAME', l2_val=2e-4):\n",
        "    return tf.keras.layers.Conv2D(\n",
        "          filters,\n",
        "          kernel_size,\n",
        "          padding=padding,\n",
        "          activation=tf.nn.relu,\n",
        "          bias_regularizer=tf.keras.regularizers.L2(l2_val),\n",
        "          kernel_regularizer=tf.keras.regularizers.L2(l2_val))\n",
        "\n",
        "def max_pool():\n",
        "    return tf.keras.layers.MaxPooling2D(\n",
        "        pool_size=[2, 2],\n",
        "        strides=[2, 2],\n",
        "        #padding='same',\n",
        "    )\n",
        "def avg_pool():\n",
        "    return tf.keras.layers.AveragePooling2D(\n",
        "        pool_size=[2, 2],\n",
        "        strides=[2, 2],\n",
        "        #padding='same',\n",
        "    )\n",
        "\n",
        "def dense(units, activation, l2_val=2e-4):\n",
        "    return tf.keras.layers.Dense(\n",
        "        units,\n",
        "        activation,\n",
        "        bias_regularizer=tf.keras.regularizers.L2(l2_val),\n",
        "        kernel_regularizer=tf.keras.regularizers.L2(l2_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oCbBDK6PTRNf"
      },
      "outputs": [],
      "source": [
        "def conv2d_variational(filters, kernel_size, strides=(1, 1), padding='SAME', kernel_posterior_fn=None, kernel_prior_fn=None, bias_prior_fn=None, kernel_divergence_fn=None):\n",
        "    \"\"\"Convenience wrapper for conv layers.\"\"\"\n",
        "    if bias_prior_fn==None:\n",
        "      bias_prior_fn = kernel_prior_fn\n",
        "    \n",
        "    return tfp.layers.Convolution2DReparameterization(\n",
        "          filters,\n",
        "          kernel_size,\n",
        "          padding=padding,\n",
        "          activation=tf.nn.relu,\n",
        "          kernel_posterior_fn=kernel_posterior_fn,\n",
        "          kernel_prior_fn=kernel_prior_fn,\n",
        "          kernel_divergence_fn=kernel_divergence_fn,\n",
        "          bias_posterior_fn = kernel_posterior_fn,\n",
        "          bias_prior_fn = bias_prior_fn,\n",
        "          bias_divergence_fn=kernel_divergence_fn)\n",
        "\n",
        "\n",
        "def dense_variational(units, activation, kernel_posterior_fn=None, kernel_prior_fn=None, bias_prior_fn=None, kernel_divergence_fn=None):\n",
        "    if bias_prior_fn==None:\n",
        "      bias_prior_fn = kernel_prior_fn\n",
        "\n",
        "    return tfp.layers.DenseReparameterization(\n",
        "        units,\n",
        "        activation,\n",
        "        kernel_posterior_fn=kernel_posterior_fn,\n",
        "        kernel_prior_fn=kernel_prior_fn,\n",
        "        kernel_divergence_fn=kernel_divergence_fn,\n",
        "        bias_posterior_fn = kernel_posterior_fn,\n",
        "        bias_prior_fn = bias_prior_fn,\n",
        "        bias_divergence_fn=kernel_divergence_fn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHhNiKBIwziq"
      },
      "source": [
        "## LeNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMo13XRwNtzv"
      },
      "source": [
        "### Variational"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "RE-IGUORIEDn"
      },
      "outputs": [],
      "source": [
        "class LeNet_Variational(Sequential):\n",
        "  def __init__(self, input_shape, nb_classes,\n",
        "               kernel_posterior_fn=make_normal_posterior(),\n",
        "               kernel_prior_fn=make_normal_prior(1.0),\n",
        "               kernel_divergence_fn=lambda q, p, ignore: kl_lib.kl_divergence(q, p)/(x_train.shape[0]*lamb),\n",
        "               gamma=1.0):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.weight_layers = [0, 2, 4, 6, 7]\n",
        "\n",
        "    self.add(tf.keras.layers.Input(shape=input_shape))\n",
        "\n",
        "    self.add(conv2d_variational(6, 5, padding = \"same\", kernel_posterior_fn=kernel_posterior_fn, \n",
        "                                kernel_prior_fn=kernel_prior_fn, \n",
        "                                kernel_divergence_fn=kernel_divergence_fn))\n",
        "    self.add(avg_pool())\n",
        "\n",
        "    self.add(conv2d_variational(16, 5, padding=\"same\", kernel_posterior_fn=kernel_posterior_fn, \n",
        "                                kernel_prior_fn=kernel_prior_fn, \n",
        "                                kernel_divergence_fn=kernel_divergence_fn))\n",
        "    self.add(avg_pool())\n",
        "\n",
        "    self.add(Flatten())\n",
        "\n",
        "\n",
        "    self.add(dense_variational(120, tf.nn.relu, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "\n",
        "\n",
        "    self.add(dense_variational(84, tf.nn.relu, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "\n",
        "    self.add(dense_variational(nb_classes, None, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "    \n",
        "    self.add(Lambda(lambda x: x*gamma))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet_Variational_Large(Sequential):\n",
        "  def __init__(self, input_shape, nb_classes,\n",
        "               kernel_posterior_fn=make_normal_posterior(),\n",
        "               kernel_prior_fn=make_normal_prior(1.0),\n",
        "               kernel_divergence_fn=lambda q, p, ignore: kl_lib.kl_divergence(q, p)/(x_train.shape[0]*lamb),\n",
        "               gamma=1.0):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.weight_layers = [0, 2, 4, 6, 7]\n",
        "\n",
        "    self.add(tf.keras.layers.Input(shape=input_shape))\n",
        "\n",
        "    self.add(conv2d_variational(6, 5, kernel_posterior_fn=kernel_posterior_fn, \n",
        "                                kernel_prior_fn=kernel_prior_fn, \n",
        "                                kernel_divergence_fn=kernel_divergence_fn))\n",
        "    self.add(max_pool())\n",
        "\n",
        "    self.add(conv2d_variational(16, 5, kernel_posterior_fn=kernel_posterior_fn, \n",
        "                                kernel_prior_fn=kernel_prior_fn, \n",
        "                                kernel_divergence_fn=kernel_divergence_fn))\n",
        "    self.add(max_pool())\n",
        "\n",
        "    self.add(conv2d_variational(120, 5, kernel_posterior_fn=kernel_posterior_fn, \n",
        "                                kernel_prior_fn=kernel_prior_fn, \n",
        "                                kernel_divergence_fn=kernel_divergence_fn))\n",
        "\n",
        "    self.add(Flatten())\n",
        "\n",
        "    self.add(dense_variational(84, tf.nn.relu, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "\n",
        "    self.add(dense_variational(nb_classes, None, \n",
        "                               kernel_posterior_fn=kernel_posterior_fn, \n",
        "                               kernel_prior_fn=kernel_prior_fn, \n",
        "                               kernel_divergence_fn=kernel_divergence_fn))\n",
        "    \n",
        "    self.add(Lambda(lambda x: x*gamma))\n",
        "     "
      ],
      "metadata": {
        "id": "USVR5Cnxmoth"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train, x_test, y_test = load_data('mnist')\n",
        "lamb=1\n",
        "model_variational = LeNet_Variational_Large(x_train[0].shape, num_classes)\n",
        "model_variational.summary()"
      ],
      "metadata": {
        "id": "9r6WjDAwXU6h",
        "outputId": "561e1c87-48d2-4443-8cf0-af4b519125b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/tensorflow_probability/python/layers/util.py:95: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  loc = add_variable_fn(\n",
            "/usr/local/lib/python3.9/dist-packages/tensorflow_probability/python/layers/util.py:105: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  untransformed_scale = add_variable_fn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"le_net__variational__large\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_reparameterization (  (None, 28, 28, 6)        312       \n",
            " Conv2DReparameterization)                                       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 6)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_reparameterization_1  (None, 14, 14, 16)       4832      \n",
            "  (Conv2DReparameterization)                                     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 16)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_reparameterization_2  (None, 7, 7, 120)        96240     \n",
            "  (Conv2DReparameterization)                                     \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 5880)              0         \n",
            "                                                                 \n",
            " dense_reparameterization (D  (None, 84)               988008    \n",
            " enseReparameterization)                                         \n",
            "                                                                 \n",
            " dense_reparameterization_1   (None, 10)               1700      \n",
            " (DenseReparameterization)                                       \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 10)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,091,092\n",
            "Trainable params: 1,091,092\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwGNoPYQQdNE"
      },
      "source": [
        "# Training Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qXUVl6tWoO00"
      },
      "outputs": [],
      "source": [
        "cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.0)\n",
        "\n",
        "@tf.function\n",
        "def ece(y_true,y_pred):\n",
        "  return tfp.stats.expected_calibration_error(10,logits=y_pred, labels_true=tf.argmax(y_true,axis=1))\n",
        "\n",
        "# Place the logs in a timestamped subdirectory\n",
        "# This allows to easy select different training runs\n",
        "# In order not to overwrite some data, it is useful to have a name with a timestamp\n",
        "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Specify the callback object\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "# tf.keras.callback.TensorBoard ensures that logs are created and stored\n",
        "# We need to pass callback object to the fit method\n",
        "# The way to do this is by passing the list of callback objects, which is in our case just one\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_categorical_crossentropy\", patience=10, restore_best_weights=True)\n",
        "\n",
        "adam_opt = tf.keras.optimizers.Adam(0.001)\n",
        "\n",
        "def schedule(epoch, lr):\n",
        "  if (epoch<25):\n",
        "    return 0.001\n",
        "  elif epoch<50:\n",
        "    return 0.0001\n",
        "  else:\n",
        "    return 0.00001\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(schedule)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iCCKNSDXfTis"
      },
      "outputs": [],
      "source": [
        "class BMA_Model(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, model, mc_samples):\n",
        "    super().__init__()\n",
        "    self.model = model\n",
        "    self.mc_samples = mc_samples\n",
        "  \n",
        "  def call(self, inputs, training=False):\n",
        "    list_models =[self.model(inputs) for i in range(self.mc_samples)]\n",
        "    stack_models = tf.stack(list_models,axis=1)\n",
        "    return tfp.math.reduce_logmeanexp(stack_models,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPt69y9ZcusY"
      },
      "source": [
        "# Variational Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "7OWzof1fmtBb"
      },
      "outputs": [],
      "source": [
        "# experiment settings\n",
        "data_use=\"mnist\"\n",
        "model_used=\"lenet\"\n",
        "num_posterior_samples=10\n",
        "n_epochs=100\n",
        "batch_size=100\n",
        "lr=0.001\n",
        "\n",
        "seeds=[15]#,24]\n",
        "\n",
        "# lambs for likelihood tempering\n",
        "lambs=np.linspace(0.5, 3.5, 7)\n",
        "lambs = [1.0]\n",
        "\n",
        "# label_smoothing\n",
        "# label_smoothing=0.0\n",
        "# cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=label_smoothing)\n",
        "\n",
        "# label noise\n",
        "label_noises=[0.0] #0.1,0.2,0.3]\n",
        "\n",
        "# smooth softmax\n",
        "gammas=[1.0]#,5.0,10.0]\n",
        "\n",
        "# data augmentation:\n",
        "#   - None: No data augmentation\n",
        "#   - Standard: \n",
        "#   - Replacement\n",
        "#   - Noise\n",
        "d_augmentation = [\"Replacement\"]\n",
        "\n",
        "# prior\n",
        "prior_scales=[1.0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "RbTGSR9jQje6"
      },
      "outputs": [],
      "source": [
        "# x_train_fashion, y_train_fashion, x_test_fashion, y_test_fashion = load_data('fashion_mnist')\n",
        "x_train_mnist, y_train_mnist, x_test_mnist, y_test_mnist = load_data('mnist')\n",
        "\n",
        "x_train_mnist = x_train_mnist[:10000]\n",
        "y_train_mnist = y_train_mnist[:10000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "n4i8AnQ6Qt-3",
        "outputId": "a587f320-7842-40e0-c2c6-3e4cf4b3d381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "1.0\n",
            "Epoch 1/100\n",
            "100/100 [==============================] - 8s 29ms/step - loss: 137.6413 - accuracy: 0.4261 - categorical_crossentropy: 1.6564 - val_loss: 134.7576 - val_accuracy: 0.7090 - val_categorical_crossentropy: 0.8774\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 2s 20ms/step - loss: 132.5437 - accuracy: 0.7691 - categorical_crossentropy: 0.6990 - val_loss: 130.2705 - val_accuracy: 0.8368 - val_categorical_crossentropy: 0.5104\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 128.1282 - accuracy: 0.8627 - categorical_crossentropy: 0.4300 - val_loss: 125.9380 - val_accuracy: 0.8930 - val_categorical_crossentropy: 0.3481\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 123.8221 - accuracy: 0.9034 - categorical_crossentropy: 0.3111 - val_loss: 121.6549 - val_accuracy: 0.9179 - val_categorical_crossentropy: 0.2704\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 2s 20ms/step - loss: 119.5171 - accuracy: 0.9223 - categorical_crossentropy: 0.2307 - val_loss: 117.3722 - val_accuracy: 0.9320 - val_categorical_crossentropy: 0.2298\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 115.2387 - accuracy: 0.9359 - categorical_crossentropy: 0.2025 - val_loss: 113.1003 - val_accuracy: 0.9382 - val_categorical_crossentropy: 0.2143\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 2s 20ms/step - loss: 110.9373 - accuracy: 0.9483 - categorical_crossentropy: 0.1633 - val_loss: 108.8198 - val_accuracy: 0.9372 - val_categorical_crossentropy: 0.2014\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 106.6511 - accuracy: 0.9553 - categorical_crossentropy: 0.1447 - val_loss: 104.5258 - val_accuracy: 0.9493 - val_categorical_crossentropy: 0.1726\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 102.3993 - accuracy: 0.9528 - categorical_crossentropy: 0.1497 - val_loss: 100.2805 - val_accuracy: 0.9506 - val_categorical_crossentropy: 0.1718\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 98.1521 - accuracy: 0.9572 - categorical_crossentropy: 0.1353 - val_loss: 96.0648 - val_accuracy: 0.9475 - val_categorical_crossentropy: 0.1771\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 93.9277 - accuracy: 0.9631 - categorical_crossentropy: 0.1174 - val_loss: 91.8616 - val_accuracy: 0.9531 - val_categorical_crossentropy: 0.1632\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 89.7662 - accuracy: 0.9620 - categorical_crossentropy: 0.1227 - val_loss: 87.7179 - val_accuracy: 0.9535 - val_categorical_crossentropy: 0.1607\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 85.6518 - accuracy: 0.9621 - categorical_crossentropy: 0.1180 - val_loss: 83.6527 - val_accuracy: 0.9518 - val_categorical_crossentropy: 0.1728\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 81.6109 - accuracy: 0.9629 - categorical_crossentropy: 0.1205 - val_loss: 79.6419 - val_accuracy: 0.9533 - val_categorical_crossentropy: 0.1673\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 77.6378 - accuracy: 0.9629 - categorical_crossentropy: 0.1153 - val_loss: 75.7158 - val_accuracy: 0.9520 - val_categorical_crossentropy: 0.1720\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 73.7499 - accuracy: 0.9631 - categorical_crossentropy: 0.1171 - val_loss: 71.8783 - val_accuracy: 0.9516 - val_categorical_crossentropy: 0.1805\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 69.9285 - accuracy: 0.9654 - categorical_crossentropy: 0.1018 - val_loss: 68.1393 - val_accuracy: 0.9458 - val_categorical_crossentropy: 0.2045\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 66.2276 - accuracy: 0.9645 - categorical_crossentropy: 0.1127 - val_loss: 64.4613 - val_accuracy: 0.9527 - val_categorical_crossentropy: 0.1854\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 62.6319 - accuracy: 0.9642 - categorical_crossentropy: 0.1199 - val_loss: 60.9022 - val_accuracy: 0.9518 - val_categorical_crossentropy: 0.1687\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 59.1551 - accuracy: 0.9627 - categorical_crossentropy: 0.1253 - val_loss: 57.5031 - val_accuracy: 0.9492 - val_categorical_crossentropy: 0.1896\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - 2s 20ms/step - loss: 55.7974 - accuracy: 0.9626 - categorical_crossentropy: 0.1267 - val_loss: 54.2077 - val_accuracy: 0.9502 - val_categorical_crossentropy: 0.1893\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - 2s 20ms/step - loss: 52.5583 - accuracy: 0.9622 - categorical_crossentropy: 0.1185 - val_loss: 51.0440 - val_accuracy: 0.9513 - val_categorical_crossentropy: 0.1936\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 49.4634 - accuracy: 0.9617 - categorical_crossentropy: 0.1288 - val_loss: 47.9964 - val_accuracy: 0.9523 - val_categorical_crossentropy: 0.1866\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 46.4995 - accuracy: 0.9571 - categorical_crossentropy: 0.1406 - val_loss: 45.1021 - val_accuracy: 0.9461 - val_categorical_crossentropy: 0.1988\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 43.6579 - accuracy: 0.9588 - categorical_crossentropy: 0.1385 - val_loss: 42.3321 - val_accuracy: 0.9454 - val_categorical_crossentropy: 0.2014\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 40.9519 - accuracy: 0.9589 - categorical_crossentropy: 0.1364 - val_loss: 39.6970 - val_accuracy: 0.9465 - val_categorical_crossentropy: 0.1999\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - 1s 13ms/step - loss: 38.3848 - accuracy: 0.9578 - categorical_crossentropy: 0.1392 - val_loss: 37.2061 - val_accuracy: 0.9472 - val_categorical_crossentropy: 0.2122\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - 2s 15ms/step - loss: 35.9520 - accuracy: 0.9568 - categorical_crossentropy: 0.1400 - val_loss: 34.8363 - val_accuracy: 0.9451 - val_categorical_crossentropy: 0.2066\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - 2s 17ms/step - loss: 33.6682 - accuracy: 0.9565 - categorical_crossentropy: 0.1521 - val_loss: 32.6157 - val_accuracy: 0.9451 - val_categorical_crossentropy: 0.2108\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - 2s 20ms/step - loss: 31.5090 - accuracy: 0.9542 - categorical_crossentropy: 0.1491 - val_loss: 30.5421 - val_accuracy: 0.9402 - val_categorical_crossentropy: 0.2252\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 29.4800 - accuracy: 0.9575 - categorical_crossentropy: 0.1420 - val_loss: 28.5803 - val_accuracy: 0.9480 - val_categorical_crossentropy: 0.2200\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 27.5903 - accuracy: 0.9581 - categorical_crossentropy: 0.1459 - val_loss: 26.7433 - val_accuracy: 0.9469 - val_categorical_crossentropy: 0.2123\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 25.8272 - accuracy: 0.9560 - categorical_crossentropy: 0.1503 - val_loss: 25.0494 - val_accuracy: 0.9456 - val_categorical_crossentropy: 0.2238\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 24.1861 - accuracy: 0.9530 - categorical_crossentropy: 0.1558 - val_loss: 23.4694 - val_accuracy: 0.9475 - val_categorical_crossentropy: 0.2293\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 22.6777 - accuracy: 0.9520 - categorical_crossentropy: 0.1722 - val_loss: 22.0077 - val_accuracy: 0.9442 - val_categorical_crossentropy: 0.2317\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 21.2632 - accuracy: 0.9511 - categorical_crossentropy: 0.1665 - val_loss: 20.6439 - val_accuracy: 0.9451 - val_categorical_crossentropy: 0.2226\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 19.9489 - accuracy: 0.9548 - categorical_crossentropy: 0.1565 - val_loss: 19.3916 - val_accuracy: 0.9458 - val_categorical_crossentropy: 0.2245\n",
            "Epoch 38/100\n",
            " 10/100 [==>...........................] - ETA: 0s - loss: 19.2977 - accuracy: 0.9440 - categorical_crossentropy: 0.1849"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-c23755b0a3eb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             model_variational.fit(train_loader, \n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "model_variational=[]\n",
        "for prior_scale in prior_scales: \n",
        "  for gamma in gammas:\n",
        "    for label_noise in label_noises:\n",
        "      for da in d_augmentation:\n",
        "\n",
        "        setting_name=f\"prior_scale_{prior_scale}_label_noise_{label_noise}_smooth_softmax_{gamma}_data_augmentation_{da}\" \n",
        "\n",
        "        x_train = x_train_mnist\n",
        "        y_train = y_train_mnist\n",
        "        x_test = x_test_mnist\n",
        "        y_test = y_test_mnist\n",
        "\n",
        "        input_shape=x_train[0].shape\n",
        "\n",
        "        if label_noise > 0.0:\n",
        "          import random\n",
        "          # train\n",
        "          ind_noisy=random.sample(range(1, y_train.shape[0]), int(y_train.shape[0]*label_noise))\n",
        "          for ind in ind_noisy:\n",
        "            false_label=random.sample(range(1,y_train.shape[1]),1)[0]\n",
        "            while y_train[ind, false_label] != 1.0:\n",
        "              temp=np.zeros((y_train.shape[1]))\n",
        "              temp[false_label]=1.0\n",
        "              y_train[ind, :]=temp\n",
        "              break\n",
        "            else:\n",
        "              false_label=random.sample(range(1,y_train.shape[1]),1)[0]\n",
        "          # test\n",
        "          ind_noisy=random.sample(range(1, y_test.shape[0]), int(y_test.shape[0]*label_noise))\n",
        "          for ind in ind_noisy:\n",
        "            false_label=random.sample(range(1,y_test.shape[1]),1)[0]\n",
        "            while y_test[ind, false_label] != 1.0:\n",
        "              temp=np.zeros((y_test.shape[1]))\n",
        "              temp[false_label]=1.0\n",
        "              y_test[ind, :]=temp\n",
        "              break\n",
        "            else:\n",
        "              false_label=random.sample(range(1,y_test.shape[1]),1)[0]\n",
        "\n",
        "        from tensorflow import keras\n",
        "        from tensorflow.keras import layers\n",
        "\n",
        "        if da == \"None\":\n",
        "\n",
        "            train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "            train_loader = train_loader.batch(batch_size)\n",
        "\n",
        "        if da == \"Standard\":\n",
        "            # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
        "            data_augmentation = keras.Sequential(\n",
        "                [\n",
        "                    layers.RandomFlip(\"horizontal\"),\n",
        "                    layers.RandomRotation(0.1),\n",
        "                    layers.RandomZoom(0.1),\n",
        "                ]\n",
        "            )\n",
        "            # Create a tf.data pipeline of augmented images (and their labels)\n",
        "            train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "            train_loader = train_loader.batch(batch_size).map(lambda x, y: (data_augmentation(x), y))\n",
        "\n",
        "\n",
        "        if da == \"Noise\":\n",
        "\n",
        "            rng = np.random.default_rng(1234)\n",
        "            n_features = x_train.shape[1]*x_train.shape[2]*x_train.shape[3]\n",
        "            idx = np.random.choice(np.arange(0, n_features, 1), n_features // 5 * 4, replace = False)\n",
        "            perm = rng.permutation(idx)\n",
        "\n",
        "            aux =  x_train.reshape(x_train.shape[0], -1)\n",
        "            aux[:, idx] = aux[:, perm]\n",
        "\n",
        "            x_train = aux.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], x_train.shape[3])\n",
        "\n",
        "            # Create a tf.data pipeline of augmented images (and their labels)\n",
        "            train_loader = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "            train_loader = train_loader.batch(batch_size)\n",
        "\n",
        "        if da == \"Replacement\":\n",
        "\n",
        "            rng = np.random.default_rng(1234)\n",
        "\n",
        "            idx = np.random.choice(np.arange(0, x_train.shape[0], 1), x_train.shape[0], replace= True)\n",
        "\n",
        "            # Create a tf.data pipeline of augmented images (and their labels)\n",
        "            train_loader = tf.data.Dataset.from_tensor_slices((x_train[idx], y_train[idx]))\n",
        "            train_loader = train_loader.batch(batch_size)\n",
        "        \n",
        "        \n",
        "        input_shape = x_train.shape[1:]\n",
        "        results={}\n",
        "\n",
        "        for seed in seeds:\n",
        "          print(seed)\n",
        "          results[seed]={} # a dict, with the structure, dict_results[lamb]=[log_ps_train, log_ps_test, metrics_bma]\n",
        "          for lamb in lambs:\n",
        "            \n",
        "            print(lamb)\n",
        "            results[seed][lamb]=[]\n",
        "\n",
        "            tf.keras.utils.set_random_seed(seed)\n",
        "\n",
        "            model_variational = LeNet_Variational_Large(input_shape, num_classes,\n",
        "                                                      kernel_prior_fn=make_normal_prior(prior_scale),\n",
        "                                                      kernel_divergence_fn=lambda q, p, ignore: kl_lib.kl_divergence(q, p)/(x_train.shape[0]*lamb),\n",
        "                                                      gamma=gamma)\n",
        "\n",
        "            model_variational.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
        "                            loss = cce,\n",
        "                            metrics=['accuracy', cce]\n",
        "                            )\n",
        "\n",
        "\n",
        "            model_variational.fit(train_loader, \n",
        "                    epochs=n_epochs, \n",
        "                    validation_data = (x_test, y_test),\n",
        "                    # workers=8,\n",
        "                    verbose = 1)\n",
        "            \n",
        "\n",
        "            log_ps_train=[]\n",
        "            log_ps_test=[]\n",
        "            metrics_gibbs_train=[]\n",
        "            metrics_gibbs_test=[]\n",
        "            metrics_bayes_train=[]\n",
        "            metrics_bayes_test=[]\n",
        "\n",
        "            # loop through posterior samples\n",
        "            for i in range(num_posterior_samples):\n",
        "              \n",
        "              # compute the categorical dist. and log_p for train data\n",
        "              x_train_augmented = np.concatenate([x for x, y in train_loader.as_numpy_iterator()])\n",
        "              y_train_augmented = np.concatenate([y for x, y in train_loader.as_numpy_iterator()])\n",
        "              p_train=tfp.distributions.Categorical(logits=model_variational(x_train_augmented))\n",
        "              log_p_train=p_train.log_prob(tf.argmax(y_train_augmented, axis=1))\n",
        "\n",
        "              log_ps_train.append(log_p_train)\n",
        "\n",
        "              # compute the categorical dist. and log_p for test data\n",
        "              p_test=tfp.distributions.Categorical(logits=model_variational(x_test)) \n",
        "              log_p_test=p_test.log_prob(tf.argmax(y_test, axis=1))\n",
        "              log_ps_test.append(log_p_test)\n",
        "\n",
        "\n",
        "\n",
        "              # compute gibbs-based metrics\n",
        "              metrics_gibbs_train.append(model_variational.evaluate(x_train, y_train, batch_size=batch_size))\n",
        "              metrics_gibbs_test.append(model_variational.evaluate(x_test, y_test, batch_size=batch_size))\n",
        "\n",
        "            # compute bayes-based metric\n",
        "            bma_model=BMA_Model(model_variational, num_posterior_samples)\n",
        "            bma_model.compile(metrics=['accuracy', cce])\n",
        "            metrics_bayes_train.append(bma_model.evaluate(x_train, y_train, batch_size=batch_size))\n",
        "            metrics_bayes_test.append(bma_model.evaluate(x_test, y_test, batch_size=batch_size))\n",
        "\n",
        "            !nvidia-smi\n",
        "            \n",
        "            # save log_p for train data\n",
        "            results[seed][lamb].append(tf.stack(log_ps_train,axis=1).numpy())\n",
        "            # save log_p for test data \n",
        "            results[seed][lamb].append(tf.stack(log_ps_test,axis=1).numpy())\n",
        "            # save gibbs-based metric\n",
        "            results[seed][lamb].append(metrics_gibbs_train)\n",
        "            results[seed][lamb].append(metrics_gibbs_test)\n",
        "            # save bayes-based metric\n",
        "            results[seed][lamb].append(metrics_bayes_train)\n",
        "            results[seed][lamb].append(metrics_bayes_test)\n",
        "\n",
        "        with open(f'{setting_name}.pickle', 'wb') as handle:\n",
        "          pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjOWNM9Cla0u"
      },
      "outputs": [],
      "source": [
        "# if if_da:\n",
        "#   from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "#   datagen = ImageDataGenerator(\n",
        "#           rotation_range=10,  # Rotate the image randomly by up to 10 degrees\n",
        "#           zoom_range=0.1,     # Zoom in or out of the image randomly by up to 10%\n",
        "#           width_shift_range=0.1,  # Shift the image horizontally by up to 10%\n",
        "#           height_shift_range=0.1, # Shift the image vertically by up to 10%\n",
        "#           )\n",
        "#   datagen.fit(x_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB60tuuQzZ3q"
      },
      "outputs": [],
      "source": [
        "# if if_da:\n",
        "#   from tensorflow import keras\n",
        "#   from tensorflow.keras import layers\n",
        "\n",
        "#   # Create a data augmentation stage with horizontal flipping, rotations, zooms\n",
        "#   data_augmentation = keras.Sequential(\n",
        "#       [\n",
        "#           layers.RandomFlip(\"horizontal\"),\n",
        "#           layers.RandomRotation(0.1),\n",
        "#           layers.RandomZoom(0.1),\n",
        "#       ]\n",
        "#   )\n",
        "\n",
        "#   input_shape = x_train.shape[1:]\n",
        "#   classes = 10\n",
        "\n",
        "#   # Create a tf.data pipeline of augmented images (and their labels)\n",
        "#   train_augmented = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "#   train_augmented = train_augmented.batch(batch_size).map(lambda x, y: (data_augmentation(x), y))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_variational.summary()"
      ],
      "metadata": {
        "id": "hgXENp57XfDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi"
      ],
      "metadata": {
        "id": "7bAlp9m5Zg9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJEY_ldvaQ83"
      },
      "outputs": [],
      "source": [
        "# files.download('prior_scale_1.0_label_noise_0.0_smooth_softmax_3.0_data_augmentation_False.pickle') "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "itHnSRJmgOXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqmjiCxDXxUO"
      },
      "outputs": [],
      "source": [
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1vfKZAloS2gJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}